{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 14:55:07.876132: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import load\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout, LSTM, Input\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def LSTM_(input_shape, **params):\n",
    "    # Define the model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Explicitly define the Input layer\n",
    "    model.add(Input(shape=input_shape[0]))\n",
    "    # Add an LSTM layer with dropout\n",
    "    model.add(LSTM(units=50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))  # Dropout of 20%\n",
    "    # Add another LSTM layer with dropout\n",
    "    model.add(LSTM(units=50))\n",
    "    model.add(Dropout(0.2))  # Dropout of 20%\n",
    "    # Add a Dense layer with 'tanh' activation to output values between -1 and 1\n",
    "    model.add(Dense(1, activation='tanh'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')   \n",
    "\n",
    "    return model, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from collections import defaultdict\n",
    "# from operator import itemgetter\n",
    "from joblib import load\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import MultiHeadAttention, GlobalAveragePooling1D, Concatenate, LayerNormalization, Dense, Conv1D, Flatten, MaxPooling1D, Dropout, LSTM, BatchNormalization, AveragePooling1D, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras_nlp.layers import SinePositionEncoding, TransformerEncoder\n",
    "from keras.regularizers import l2\n",
    "\n",
    "def Transformer1Input_(input_shape, **params):\n",
    "    \"\"\"\n",
    "    - Neural network model for dual time series with varying sequence lengths and features.\n",
    "    - Incorporates two TransformerEncoder layers, each tailored to respective input features (4 and 12 features).\n",
    "    - Applies Sine Positional Encoding for sequence order in both time series.\n",
    "    - Processes each series independently, then merges outputs.\n",
    "    - Final classification via dense layers with softmax activation.\n",
    "    - Uses Adam optimizer and sparse categorical cross-entropy loss.\n",
    "    - Adjustable for specific sequence lengths and data characteristics.\n",
    "\n",
    "    \"\"\"\n",
    "    custom_layers = {}\n",
    "\n",
    "    learning_rate = params.get(\"learning_rate\", 0.001)\n",
    "    #transformer layers for each input\n",
    "    trans_layers = params.get(\"trans_layers\", [1,1])\n",
    "    l2_reg = params.get(\"l2_reg\", 0.001)  # Added L2 regularization parameter\n",
    "\n",
    "    input_ts1 = Input(shape=input_shape[0])  # Adjust sequence_length_1 as needed\n",
    "    pos_encoding_1 = SinePositionEncoding()(input_ts1)\n",
    "    x1 = pos_encoding_1 + input_ts1\n",
    "    for _ in range(trans_layers[0]):\n",
    "        x1 = TransformerEncoder(intermediate_dim=64, num_heads=5, dropout=0.2)(x1)\n",
    "    x1 = GlobalAveragePooling1D()(x1)\n",
    "    x = Dense(64, activation='relu')(x1)\n",
    "    output = Dense(1, activation='linear')(x)  # Single output neuron with tanh activation\n",
    "    model = Model(inputs=[input_ts1], outputs=output)\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mean_squared_error'])\n",
    "\n",
    "    return model, custom_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from tcn import TCN, tcn_full_summary\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout, LSTM\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Temporal Convolution Network (https://github.com/philipperemy/keras-tcn)\n",
    "\n",
    "https://chat.openai.com/g/g-GzdUmx53z-market-strategist\n",
    "\n",
    "zatím jednoduchá TCN, v případě je možno vrstvit s return_sequences=True (jako u LSTM) a nebo Conv1D\n",
    "\"\"\"\n",
    "\n",
    "def TCN_(input_shape, **params):\n",
    "    custom_layers = {\n",
    "        'TCN': TCN\n",
    "    }\n",
    "    learning_rate = 0.001\n",
    "    if \"learning_rate\" in params:\n",
    "        learning_rate = float(params[\"learning_rate\"])\n",
    "    model = Sequential()\n",
    "    # Building the TCN model\n",
    "    model.add(\n",
    "        TCN(input_shape=input_shape[0], \n",
    "            nb_filters=128, \n",
    "            kernel_size=5, #If sequence heavily depends on t-1 and t-2, but less on the rest, then choose a kernel size of 2/3.\n",
    "            dilations=[1, 2, 4, 8], \n",
    "            padding='causal', \n",
    "            use_skip_connections=True))\n",
    "    model.add(Dense(1, activation='tanh'))  # Single output neuron with tanh activation\n",
    "    # Compile the model with a custom learning rate\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mean_squared_error'])\n",
    "    tcn_full_summary(model, expand_residual_blocks=False) \n",
    "    return model, custom_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import MultiHeadAttention, GlobalAveragePooling1D, Concatenate, LayerNormalization\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout, LSTM, BatchNormalization, AveragePooling1D, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras_nlp.layers import SinePositionEncoding, TransformerEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def load_and_analyze_data(file_path):\n",
    "    \"\"\"\n",
    "    Load the CSV file and perform initial analysis\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert Open time to datetime and set as index\n",
    "    df['Open time'] = pd.to_datetime(df['Open time'])\n",
    "    df.set_index('Open time', inplace=True)\n",
    "    \n",
    "    # Basic info about the dataset\n",
    "    #print(\"\\nDataset Info:\")\n",
    "    #print(df.info())\n",
    "    \n",
    "    # Get feature statistics\n",
    "    #print(\"\\nFeature Statistics:\")\n",
    "    #print(df.describe())\n",
    "    \n",
    "    # Check for missing values\n",
    "    #print(\"\\nMissing Values:\")\n",
    "    #print(df.isnull().sum())\n",
    "    \n",
    "    # Calculate correlations with target using only numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    correlations = df[numeric_cols].corr()['target'].sort_values(ascending=False)\n",
    "    print(\"\\nTop 10 Feature Correlations with Target:\")\n",
    "    print(correlations.head(10))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_data_for_model(df, sequence_length=10):\n",
    "    \"\"\"\n",
    "    Prepare data for the transformer model\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    features = df.drop(['target'], axis=1)  # Open time is already set as index\n",
    "    target = df['target']\n",
    "    \n",
    "    print(f\"Number of features: {features.shape[1]}\")\n",
    "    print(f\"Number of samples: {len(features)}\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(scaled_features) - sequence_length):\n",
    "        X.append(scaled_features[i:(i + sequence_length)])\n",
    "        y.append(target.iloc[i + sequence_length])\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, scaler\n",
    "\n",
    "def train_model(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train the transformer model\n",
    "    \"\"\"\n",
    "    # Get input shape from training data\n",
    "    sequence_length = X_train.shape[1]  # Number of time steps\n",
    "    n_features = X_train.shape[2]      # Number of features\n",
    "    \n",
    "    # For Transformer1Input_ we need to provide input shape as a tuple of (sequence_length, n_features)\n",
    "    input_shape = [(sequence_length, n_features)]  # Wrapped in list as model expects single input\n",
    "    \n",
    "    print(f\"Input shape: {input_shape}\")\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Test data shape: {X_test.shape}\")\n",
    "    \n",
    "    # Initialize and compile model\n",
    "    model, _ = Transformer1Input_(input_shape)\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    train_predictions = model.predict(X_train)\n",
    "    test_predictions = model.predict(X_test)\n",
    "    \n",
    "    train_mse = mean_squared_error(y_train, train_predictions)\n",
    "    test_mse = mean_squared_error(y_test, test_predictions)\n",
    "    \n",
    "    print(f\"\\nTrain MSE: {train_mse}\")\n",
    "    print(f\"Test MSE: {test_mse}\")\n",
    "    \n",
    "    return model, history, train_predictions, test_predictions\n",
    "\n",
    "def analyze_predictions(y_true, predictions, title):\n",
    "    \"\"\"\n",
    "    Analyze model predictions\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_true, predictions, alpha=0.5)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze data\n",
    "df = load_and_analyze_data('model_data.csv')\n",
    "\n",
    "# Prepare data for model\n",
    "X_train, X_test, y_train, y_test, scaler = prepare_data_for_model(df)\n",
    "\n",
    "# Train model\n",
    "model, history, train_predictions, test_predictions = train_model(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Analyze predictions\n",
    "analyze_predictions(y_train, train_predictions.flatten(), 'Training Set Predictions vs Actual')\n",
    "analyze_predictions(y_test, test_predictions.flatten(), 'Test Set Predictions vs Actual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression using NN - LSTM + Transformed Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "class TransformerRollingWindowTrainer:\n",
    "    def __init__(self, train_days=5, test_days=1, sequence_length=10):\n",
    "        self.train_days = train_days\n",
    "        self.test_days = test_days\n",
    "        self.window_size = train_days + test_days\n",
    "        self.sequence_length = sequence_length\n",
    "        self.results = []\n",
    "        \n",
    "    def get_day_groups(self, df):\n",
    "        \"\"\"Group data by days using datetime index\"\"\"\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "        return df.groupby(df.index.date)\n",
    "    \n",
    "    def prepare_sequences(self, data):\n",
    "        \"\"\"Prepare sequences for transformer model\"\"\"\n",
    "        features = data.drop('target', axis=1)\n",
    "        target = data['target']\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        scaled_features = scaler.fit_transform(features)\n",
    "        \n",
    "        X, y = [], []\n",
    "        for i in range(len(scaled_features) - self.sequence_length):\n",
    "            X.append(scaled_features[i:(i + self.sequence_length)])\n",
    "            y.append(target.iloc[i + self.sequence_length])\n",
    "        \n",
    "        return np.array(X), np.array(y), scaler\n",
    "    \n",
    "    def create_model(self, input_shape, learning_rate=0.001):\n",
    "        \"\"\"Create and compile transformer model\"\"\"\n",
    "        # Get the input shape dimensions\n",
    "        sequence_length, n_features = input_shape\n",
    "        \n",
    "        input_shape = [(sequence_length, n_features)]\n",
    "        model, _ = TCN_(input_shape)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def plot_confusion_matrix(self, cm, title, ax=None):\n",
    "        \"\"\"Plot confusion matrix using seaborn\"\"\"\n",
    "        if ax is None:\n",
    "            _, ax = plt.subplots(figsize=(6, 5))\n",
    "            \n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                   xticklabels=['Down', 'Up'],\n",
    "                   yticklabels=['Down', 'Up'])\n",
    "        ax.set_title(title)\n",
    "        ax.set_ylabel('Actual')\n",
    "        ax.set_xlabel('Predicted')\n",
    "    \n",
    "    def calculate_confusion_metrics(self, cm):\n",
    "        \"\"\"Calculate metrics from confusion matrix\"\"\"\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        total = np.sum(cm)\n",
    "        \n",
    "        accuracy = (tp + tn) / total\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1,\n",
    "            'Total Samples': total\n",
    "        }\n",
    "    \n",
    "    def run_rolling_window(self, df, n_iterations=None):\n",
    "        \"\"\"Run rolling window analysis\"\"\"\n",
    "        self.results = []\n",
    "        self.iteration_metrics = []  # Store metrics for each iteration\n",
    "        \n",
    "        # Group data by days\n",
    "        day_groups = self.get_day_groups(df)\n",
    "        unique_days = list(day_groups.groups.keys())\n",
    "        \n",
    "        # Calculate maximum possible iterations\n",
    "        max_iterations = len(unique_days) - self.train_days\n",
    "        \n",
    "        if max_iterations <= 0:\n",
    "            raise ValueError(f\"Dataset contains {len(unique_days)} days, but {self.train_days} training days are required.\")\n",
    "        \n",
    "        # Validate/set n_iterations\n",
    "        if n_iterations is None:\n",
    "            n_iterations = max_iterations\n",
    "        else:\n",
    "            n_iterations = min(n_iterations, max_iterations)\n",
    "            \n",
    "        print(f\"Running {n_iterations} iterations out of maximum possible {max_iterations} iterations\")\n",
    "        \n",
    "        for i in range(n_iterations):\n",
    "            print(f\"\\nIteration {i+1}/{n_iterations}\")\n",
    "            \n",
    "            # Get indices for train and test days\n",
    "            train_start_idx = i\n",
    "            train_end_idx = train_start_idx + self.train_days\n",
    "            test_idx = train_end_idx\n",
    "            \n",
    "            if test_idx >= len(unique_days):\n",
    "                print(\"Reached end of data\")\n",
    "                break\n",
    "                \n",
    "            # Get train and test data\n",
    "            train_days = unique_days[train_start_idx:train_end_idx]\n",
    "            test_day = unique_days[test_idx]\n",
    "            \n",
    "            train_data = pd.concat([day_groups.get_group(day) for day in train_days])\n",
    "            test_data = day_groups.get_group(test_day)\n",
    "            \n",
    "            print(f\"Training days: {train_days[0]} to {train_days[-1]}\")\n",
    "            print(f\"Test day: {test_day}\")\n",
    "            \n",
    "            # Prepare sequences\n",
    "            X_train, y_train, scaler = self.prepare_sequences(train_data)\n",
    "            X_test, y_test, _ = self.prepare_sequences(test_data)\n",
    "            \n",
    "            # Create and train model\n",
    "            model = self.create_model(input_shape=(self.sequence_length, X_train.shape[2]))\n",
    "            \n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_split=0.2,\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions = model.predict(X_test)\n",
    "            \n",
    "            # Calculate iteration metrics\n",
    "            iter_predictions = predictions.flatten()\n",
    "            iter_true = y_test\n",
    "            \n",
    "            # Calculate regression metrics for this iteration\n",
    "            iter_rmse = np.sqrt(mean_squared_error(iter_true, iter_predictions))\n",
    "            iter_mae = mean_absolute_error(iter_true, iter_predictions)\n",
    "            \n",
    "            # Calculate directional metrics for this iteration\n",
    "            iter_actual_dir = (iter_true > 0).astype(int)\n",
    "            iter_pred_dir = (iter_predictions > 0).astype(int)\n",
    "            \n",
    "            iter_cm = confusion_matrix(iter_actual_dir, iter_pred_dir)\n",
    "            iter_metrics = self.calculate_confusion_metrics(iter_cm)\n",
    "            \n",
    "            # Store iteration metrics\n",
    "            self.iteration_metrics.append({\n",
    "                'iteration': i + 1,\n",
    "                'train_start': train_days[0],\n",
    "                'train_end': train_days[-1],\n",
    "                'test_day': test_day,\n",
    "                'rmse': iter_rmse,\n",
    "                'mae': iter_mae,\n",
    "                'confusion_matrix': iter_cm,\n",
    "                **iter_metrics\n",
    "            })\n",
    "            \n",
    "            # Print iteration metrics\n",
    "            print(f\"\\nIteration {i+1} Metrics:\")\n",
    "            print(f\"RMSE: {iter_rmse:.4f}\")\n",
    "            print(f\"MAE: {iter_mae:.4f}\")\n",
    "            print(f\"Accuracy: {iter_metrics['Accuracy']:.4f}\")\n",
    "            print(f\"Precision: {iter_metrics['Precision']:.4f}\")\n",
    "            print(f\"Recall: {iter_metrics['Recall']:.4f}\")\n",
    "            print(f\"F1 Score: {iter_metrics['F1 Score']:.4f}\")\n",
    "            print(\"\\nConfusion Matrix:\")\n",
    "            print(iter_cm)\n",
    "            \n",
    "            # Store results\n",
    "            for idx, (pred, true) in enumerate(zip(predictions.flatten(), y_test)):\n",
    "                self.results.append({\n",
    "                    'iteration': i + 1,\n",
    "                    'timestamp': test_data.index[idx + self.sequence_length],\n",
    "                    'date': test_data.index[idx + self.sequence_length].date(),\n",
    "                    'time': test_data.index[idx + self.sequence_length].time(),\n",
    "                    'actual': true,\n",
    "                    'predicted': pred,\n",
    "                    'train_start': train_days[0],\n",
    "                    'train_end': train_days[-1]\n",
    "                })\n",
    "        \n",
    "        # Convert results to DataFrame\n",
    "        self.results_df = pd.DataFrame(self.results)\n",
    "        return self.results_df\n",
    "    \n",
    "    def plot_iteration_metrics(self):\n",
    "        \"\"\"Plot metrics across iterations\"\"\"\n",
    "        metrics_df = pd.DataFrame(self.iteration_metrics)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Metrics Across Iterations')\n",
    "        \n",
    "        # Plot RMSE\n",
    "        axes[0, 0].plot(metrics_df['iteration'], metrics_df['rmse'], marker='o')\n",
    "        axes[0, 0].set_title('RMSE by Iteration')\n",
    "        axes[0, 0].set_xlabel('Iteration')\n",
    "        axes[0, 0].set_ylabel('RMSE')\n",
    "        \n",
    "        # Plot Accuracy\n",
    "        axes[0, 1].plot(metrics_df['iteration'], metrics_df['Accuracy'], marker='o')\n",
    "        axes[0, 1].set_title('Accuracy by Iteration')\n",
    "        axes[0, 1].set_xlabel('Iteration')\n",
    "        axes[0, 1].set_ylabel('Accuracy')\n",
    "        \n",
    "        # Plot F1 Score\n",
    "        axes[1, 0].plot(metrics_df['iteration'], metrics_df['F1 Score'], marker='o')\n",
    "        axes[1, 0].set_title('F1 Score by Iteration')\n",
    "        axes[1, 0].set_xlabel('Iteration')\n",
    "        axes[1, 0].set_ylabel('F1 Score')\n",
    "        \n",
    "        # Plot Confusion Matrices\n",
    "        ax_cm = axes[1, 1]\n",
    "        ax_cm.axis('off')\n",
    "        ax_cm.text(0.5, 0.95, 'Confusion Matrices by Iteration:', \n",
    "                  ha='center', va='top', fontsize=10)\n",
    "        \n",
    "        # Print iteration metrics in text form on the plot\n",
    "        text_content = []\n",
    "        for _, row in metrics_df.iterrows():\n",
    "            text_content.append(f\"Iteration {int(row['iteration'])}:\")\n",
    "            text_content.append(f\"Test Day: {row['test_day']}\")\n",
    "            text_content.append(f\"Accuracy: {row['Accuracy']:.4f}\")\n",
    "            text_content.append(f\"F1: {row['F1 Score']:.4f}\")\n",
    "            text_content.append(\"\")\n",
    "        \n",
    "        ax_cm.text(0.5, 0.5, '\\n'.join(text_content), \n",
    "                  ha='center', va='center', fontsize=8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        # Create iteration metrics plot\n",
    "        fig_iterations = self.plot_iteration_metrics()\n",
    "        \n",
    "        # Print summary statistics across all iterations\n",
    "        metrics_df = pd.DataFrame(self.iteration_metrics)\n",
    "        print(\"\\nSummary Statistics Across All Iterations:\")\n",
    "        print(\"\\nMean Metrics:\")\n",
    "        print(f\"RMSE: {metrics_df['rmse'].mean():.4f} (±{metrics_df['rmse'].std():.4f})\")\n",
    "        print(f\"MAE: {metrics_df['mae'].mean():.4f} (±{metrics_df['mae'].std():.4f})\")\n",
    "        print(f\"Accuracy: {metrics_df['Accuracy'].mean():.4f} (±{metrics_df['Accuracy'].std():.4f})\")\n",
    "        print(f\"F1 Score: {metrics_df['F1 Score'].mean():.4f} (±{metrics_df['F1 Score'].std():.4f})\")\n",
    "        \n",
    "        print(\"\\nMetrics Range:\")\n",
    "        print(f\"RMSE: [{metrics_df['rmse'].min():.4f}, {metrics_df['rmse'].max():.4f}]\")\n",
    "        print(f\"Accuracy: [{metrics_df['Accuracy'].min():.4f}, {metrics_df['Accuracy'].max():.4f}]\")\n",
    "        print(f\"F1 Score: [{metrics_df['F1 Score'].min():.4f}, {metrics_df['F1 Score'].max():.4f}]\")\n",
    "        \n",
    "        return fig, fig_iterations\n",
    "        \n",
    "    def evaluate_and_plot(self):\n",
    "        \"\"\"Evaluate results and create plots\"\"\"\n",
    "        if not hasattr(self, 'results_df'):\n",
    "            print(\"No results to evaluate. Run rolling_window analysis first.\")\n",
    "            return None, None\n",
    "            \n",
    "        # Calculate directional predictions\n",
    "        self.results_df['actual_direction'] = (self.results_df['actual'] > 0).astype(int)\n",
    "        self.results_df['pred_direction'] = (self.results_df['predicted'] > 0).astype(int)\n",
    "        \n",
    "        # Create main figure with subplots\n",
    "        fig_main = plt.figure(figsize=(20, 15))\n",
    "        gs = fig_main.add_gridspec(3, 2)\n",
    "        \n",
    "        # Plot 1: Actual vs Predicted over time\n",
    "        ax1 = fig_main.add_subplot(gs[0, :])\n",
    "        ax1.plot(self.results_df['timestamp'], self.results_df['actual'], \n",
    "                label='Actual', marker='o')\n",
    "        ax1.plot(self.results_df['timestamp'], self.results_df['predicted'], \n",
    "                label='Predicted', marker='o')\n",
    "        ax1.set_title('Actual vs Predicted Values Over Time')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Plot 2: Prediction Errors\n",
    "        ax2 = fig_main.add_subplot(gs[1, 0])\n",
    "        errors = self.results_df['predicted'] - self.results_df['actual']\n",
    "        ax2.plot(self.results_df['timestamp'], errors, marker='o')\n",
    "        ax2.axhline(y=0, color='r', linestyle='--')\n",
    "        ax2.set_title('Prediction Errors Over Time')\n",
    "        \n",
    "        # Plot 3: Confusion Matrix\n",
    "        ax3 = fig_main.add_subplot(gs[1, 1])\n",
    "        cm = confusion_matrix(self.results_df['actual_direction'], \n",
    "                            self.results_df['pred_direction'])\n",
    "        self.plot_confusion_matrix(cm, 'Overall Directional Confusion Matrix', ax3)\n",
    "        \n",
    "        # Calculate and print metrics\n",
    "        metrics = self.calculate_confusion_metrics(cm)\n",
    "        print(\"\\nOverall Performance Metrics:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        # Calculate regression metrics\n",
    "        rmse = np.sqrt(mean_squared_error(self.results_df['actual'], \n",
    "                                        self.results_df['predicted']))\n",
    "        mae = mean_absolute_error(self.results_df['actual'], \n",
    "                                self.results_df['predicted'])\n",
    "        \n",
    "        print(f\"\\nRegression Metrics:\")\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        \n",
    "        # Create iteration metrics figure\n",
    "        fig_iterations = plt.figure(figsize=(15, 12))\n",
    "        gs_iter = fig_iterations.add_gridspec(2, 2)\n",
    "        \n",
    "        metrics_df = pd.DataFrame(self.iteration_metrics)\n",
    "        \n",
    "        # Plot RMSE\n",
    "        ax_rmse = fig_iterations.add_subplot(gs_iter[0, 0])\n",
    "        ax_rmse.plot(metrics_df['iteration'], metrics_df['rmse'], marker='o')\n",
    "        ax_rmse.set_title('RMSE by Iteration')\n",
    "        ax_rmse.set_xlabel('Iteration')\n",
    "        ax_rmse.set_ylabel('RMSE')\n",
    "        \n",
    "        # Plot Accuracy\n",
    "        ax_acc = fig_iterations.add_subplot(gs_iter[0, 1])\n",
    "        ax_acc.plot(metrics_df['iteration'], metrics_df['Accuracy'], marker='o')\n",
    "        ax_acc.set_title('Accuracy by Iteration')\n",
    "        ax_acc.set_xlabel('Iteration')\n",
    "        ax_acc.set_ylabel('Accuracy')\n",
    "        \n",
    "        # Plot F1 Score\n",
    "        ax_f1 = fig_iterations.add_subplot(gs_iter[1, 0])\n",
    "        ax_f1.plot(metrics_df['iteration'], metrics_df['F1 Score'], marker='o')\n",
    "        ax_f1.set_title('F1 Score by Iteration')\n",
    "        ax_f1.set_xlabel('Iteration')\n",
    "        ax_f1.set_ylabel('F1 Score')\n",
    "        \n",
    "        # Plot Iteration Summary\n",
    "        ax_summary = fig_iterations.add_subplot(gs_iter[1, 1])\n",
    "        ax_summary.axis('off')\n",
    "        summary_text = []\n",
    "        for _, row in metrics_df.iterrows():\n",
    "            summary_text.append(f\"Iteration {int(row['iteration'])}:\")\n",
    "            summary_text.append(f\"Test Day: {row['test_day']}\")\n",
    "            summary_text.append(f\"Accuracy: {row['Accuracy']:.4f}\")\n",
    "            summary_text.append(f\"F1: {row['F1 Score']:.4f}\")\n",
    "            summary_text.append(\"\")\n",
    "        \n",
    "        ax_summary.text(0.5, 0.5, '\\n'.join(summary_text),\n",
    "                       ha='center', va='center', fontsize=8)\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(\"\\nSummary Statistics Across All Iterations:\")\n",
    "        print(\"\\nMean Metrics:\")\n",
    "        print(f\"RMSE: {metrics_df['rmse'].mean():.4f} (±{metrics_df['rmse'].std():.4f})\")\n",
    "        print(f\"MAE: {metrics_df['mae'].mean():.4f} (±{metrics_df['mae'].std():.4f})\")\n",
    "        print(f\"Accuracy: {metrics_df['Accuracy'].mean():.4f} (±{metrics_df['Accuracy'].std():.4f})\")\n",
    "        print(f\"F1 Score: {metrics_df['F1 Score'].mean():.4f} (±{metrics_df['F1 Score'].std():.4f})\")\n",
    "        \n",
    "        print(\"\\nMetrics Range:\")\n",
    "        print(f\"RMSE: [{metrics_df['rmse'].min():.4f}, {metrics_df['rmse'].max():.4f}]\")\n",
    "        print(f\"Accuracy: [{metrics_df['Accuracy'].min():.4f}, {metrics_df['Accuracy'].max():.4f}]\")\n",
    "        print(f\"F1 Score: [{metrics_df['F1 Score'].min():.4f}, {metrics_df['F1 Score'].max():.4f}]\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig_main, fig_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 5 iterations out of maximum possible 216 iterations\n",
      "\n",
      "Iteration 1/5\n",
      "Training days: 2023-08-01 to 2023-08-07\n",
      "Test day: 2023-08-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidbrazda/Documents/Development/python/mlroom/.venv/lib/python3.10/site-packages/tcn/tcn.py:267: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super(TCN, self).__init__(**kwargs)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.backend' has no attribute 'name_scope'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m trainer \u001b[38;5;241m=\u001b[39m TransformerRollingWindowTrainer(train_days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, test_days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, sequence_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Run analysis\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_rolling_window\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Plot results\u001b[39;00m\n\u001b[1;32m     12\u001b[0m fig_main, fig_iterations \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate_and_plot()\n",
      "Cell \u001b[0;32mIn[7], line 129\u001b[0m, in \u001b[0;36mTransformerRollingWindowTrainer.run_rolling_window\u001b[0;34m(self, df, n_iterations)\u001b[0m\n\u001b[1;32m    126\u001b[0m X_test, y_test, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_sequences(test_data)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Create and train model\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    132\u001b[0m     X_train, y_train,\n\u001b[1;32m    133\u001b[0m     validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    137\u001b[0m )\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 45\u001b[0m, in \u001b[0;36mTransformerRollingWindowTrainer.create_model\u001b[0;34m(self, input_shape, learning_rate)\u001b[0m\n\u001b[1;32m     42\u001b[0m sequence_length, n_features \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[1;32m     44\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m [(sequence_length, n_features)]\n\u001b[0;32m---> 45\u001b[0m model, _ \u001b[38;5;241m=\u001b[39m \u001b[43mTCN_\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m, in \u001b[0;36mTCN_\u001b[0;34m(input_shape, **params)\u001b[0m\n\u001b[1;32m     22\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Building the TCN model\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTCN\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnb_filters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#If sequence heavily depends on t-1 and t-2, but less on the rest, then choose a kernel size of 2/3.\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdilations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcausal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_skip_connections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m'\u001b[39m))  \u001b[38;5;66;03m# Single output neuron with tanh activation\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Compile the model with a custom learning rate\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Development/python/mlroom/.venv/lib/python3.10/site-packages/keras/src/models/sequential.py:117\u001b[0m, in \u001b[0;36mSequential.add\u001b[0;34m(self, layer, rebuild)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers\u001b[38;5;241m.\u001b[39mappend(layer)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rebuild:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_rebuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Development/python/mlroom/.venv/lib/python3.10/site-packages/keras/src/models/sequential.py:136\u001b[0m, in \u001b[0;36mSequential._maybe_rebuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m], InputLayer) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    135\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbatch_shape\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Development/python/mlroom/.venv/lib/python3.10/site-packages/keras/src/layers/layer.py:223\u001b[0m, in \u001b[0;36mLayer.__new__.<locals>.build_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(original_build_method)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mname_scope(obj\u001b[38;5;241m.\u001b[39mname, caller\u001b[38;5;241m=\u001b[39mobj):\n\u001b[0;32m--> 223\u001b[0m         \u001b[43moriginal_build_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m     signature \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(original_build_method)\n",
      "File \u001b[0;32m~/Documents/Development/python/mlroom/.venv/lib/python3.10/site-packages/keras/src/models/sequential.py:177\u001b[0m, in \u001b[0;36mSequential.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;66;03m# Can happen if shape inference is not implemented.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;66;03m# TODO: consider reverting inbound nodes on layers processed.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Development/python/mlroom/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Documents/Development/python/mlroom/.venv/lib/python3.10/site-packages/tcn/tcn.py:299\u001b[0m, in \u001b[0;36mTCN.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_blocks\u001b[38;5;241m.\u001b[39mappend(ResidualBlock(dilation_rate\u001b[38;5;241m=\u001b[39md,\n\u001b[1;32m    288\u001b[0m                                                   nb_filters\u001b[38;5;241m=\u001b[39mres_block_filters,\n\u001b[1;32m    289\u001b[0m                                                   kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m                                                   kernel_initializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_initializer,\n\u001b[1;32m    297\u001b[0m                                                   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresidual_block_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_blocks))))\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;66;03m# build newest residual block\u001b[39;00m\n\u001b[0;32m--> 299\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual_blocks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_output_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_output_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_blocks[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mres_output_shape\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# this is done to force keras to add the layers in the list to self._layers\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Development/python/mlroom/.venv/lib/python3.10/site-packages/tcn/tcn.py:87\u001b[0m, in \u001b[0;36mResidualBlock.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_shape):\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname_scope\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname):  \u001b[38;5;66;03m# name scope used to make sure weights get unique names\u001b[39;00m\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_output_shape \u001b[38;5;241m=\u001b[39m input_shape\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'keras.backend' has no attribute 'name_scope'"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('model_data.csv', parse_dates=['Open time'])\n",
    "df.set_index('Open time', inplace=True)\n",
    "\n",
    "# Initialize transformer trainer\n",
    "trainer = TransformerRollingWindowTrainer(train_days=5, test_days=1, sequence_length=1)\n",
    "\n",
    "# Run analysis\n",
    "results = trainer.run_rolling_window(df, n_iterations=5)\n",
    "\n",
    "# Plot results\n",
    "fig_main, fig_iterations = trainer.evaluate_and_plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
