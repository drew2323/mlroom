{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue based on xgboost notebook\n",
    "\n",
    "use rolling approach - build a model that can  predict the classes for one day at a time. I recommend a rolling forward process for training and testing. Like 5-12 days for training and 1 day for testing.\n",
    "\n",
    "rolling window 5 + 1\n",
    "\n",
    "same data\n",
    "\n",
    "use claude and confusion metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidbrazda/Documents/Development/python/mlroom/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "pio.renderers.default = \"notebook\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"Load and preprocess the data.\"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert open_time to datetime\n",
    "    df['Open time'] = pd.to_datetime(df['Open time'])\n",
    "    \n",
    "    # Get feature columns (excluding open_time and target)\n",
    "    feature_cols = [col for col in df.columns if col.startswith('feature_')]\n",
    "    \n",
    "    return df, feature_cols\n",
    "\n",
    "def train_test_split_by_date(df, train_days, test_days, start_idx):\n",
    "    \"\"\"Split data into training and test sets based on dates.\"\"\"\n",
    "    all_dates = df['Open time'].dt.date.unique()\n",
    "    \n",
    "    if start_idx + train_days + test_days > len(all_dates):\n",
    "        return None, None, None, None\n",
    "    \n",
    "    train_dates = all_dates[start_idx:start_idx + train_days]\n",
    "    test_dates = all_dates[start_idx + train_days:start_idx + train_days + test_days]\n",
    "    \n",
    "    train_mask = df['Open time'].dt.date.isin(train_dates)\n",
    "    test_mask = df['Open time'].dt.date.isin(test_dates)\n",
    "    \n",
    "    return df[train_mask], df[test_mask], train_dates, test_dates\n",
    "\n",
    "def evaluate_predictions(y_true, y_pred):\n",
    "    \"\"\"Calculate regression metrics.\"\"\"\n",
    "    return {\n",
    "        'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'mae': mean_absolute_error(y_true, y_pred),\n",
    "        'r2': r2_score(y_true, y_pred),\n",
    "        'mape': np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100,  # Added small constant to avoid division by zero\n",
    "        'direction_accuracy': np.mean((y_true * y_pred) > 0)  # Direction prediction accuracy\n",
    "    }\n",
    "\n",
    "def create_model(trial):\n",
    "    \"\"\"Create a model with parameters suggested by Optuna.\"\"\"\n",
    "    return XGBRegressor(\n",
    "        n_estimators=trial.suggest_int('n_estimators', 50, 300),\n",
    "        max_depth=trial.suggest_int('max_depth', 2, 8),\n",
    "        learning_rate=trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
    "        min_child_weight=trial.suggest_float('min_child_weight', 1, 7),\n",
    "        subsample=trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        colsample_bytree=trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        gamma=trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "        reg_alpha=trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "        reg_lambda=trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "def optimize_parameters(train_df, val_df, feature_cols, n_trials=50):\n",
    "    \"\"\"Optimize hyperparameters using Optuna.\"\"\"\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df['target']\n",
    "    X_val = val_df[feature_cols]\n",
    "    y_val = val_df['target']\n",
    "    \n",
    "    def objective(trial):\n",
    "        model = create_model(trial)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            early_stopping_rounds=20,\n",
    "            verbose=False\n",
    "        )\n",
    "        preds = model.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "        return rmse\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "    \n",
    "    best_model = create_model(study.best_trial)\n",
    "    best_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    return best_model, study\n",
    "\n",
    "def plot_optimization_results(study, window_idx):\n",
    "    \"\"\"Plot optimization results using Plotly.\"\"\"\n",
    "    # Plot optimization history\n",
    "    history_fig = go.Figure()\n",
    "    history_data = {\n",
    "        'number': list(range(len(study.trials))),\n",
    "        'value': [t.value for t in study.trials]\n",
    "    }\n",
    "    \n",
    "    history_fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=history_data['number'],\n",
    "            y=history_data['value'],\n",
    "            mode='lines+markers',\n",
    "            name='RMSE'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    history_fig.update_layout(\n",
    "        title=f'Optimization History - Window {window_idx}',\n",
    "        xaxis_title='Trial',\n",
    "        yaxis_title='RMSE',\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    # Plot parameter importances\n",
    "    importance = optuna.importance.get_param_importances(study)\n",
    "    importance_df = pd.DataFrame({\n",
    "        'parameter': list(importance.keys()),\n",
    "        'importance': list(importance.values())\n",
    "    }).sort_values('importance', ascending=True)\n",
    "    \n",
    "    param_fig = go.Figure()\n",
    "    param_fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=importance_df['importance'],\n",
    "            y=importance_df['parameter'],\n",
    "            orientation='h'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    param_fig.update_layout(\n",
    "        title=f'Parameter Importances - Window {window_idx}',\n",
    "        xaxis_title='Importance',\n",
    "        yaxis_title='Parameter',\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    return history_fig, param_fig\n",
    "\n",
    "def train_and_evaluate_window(train_df, test_df, feature_cols, window_idx):\n",
    "    \"\"\"Train XGBoost and evaluate for a single window with hyperparameter optimization.\"\"\"\n",
    "    global current_feature_importance\n",
    "    \n",
    "    # Split train data into train and validation for optimization\n",
    "    train_dates = train_df['Open time'].dt.date.unique()\n",
    "    val_size = max(1, len(train_dates) // 5)  # 20% of training data for validation\n",
    "    \n",
    "    val_dates = train_dates[-val_size:]\n",
    "    train_dates = train_dates[:-val_size]\n",
    "    \n",
    "    train_mask = train_df['Open time'].dt.date.isin(train_dates)\n",
    "    val_mask = train_df['Open time'].dt.date.isin(val_dates)\n",
    "    \n",
    "    opt_train_df = train_df[train_mask]\n",
    "    val_df = train_df[val_mask]\n",
    "    \n",
    "    # Optimize parameters\n",
    "    best_model, study = optimize_parameters(opt_train_df, val_df, feature_cols)\n",
    "    \n",
    "    # Create optimization plots\n",
    "    # history_fig, param_fig = plot_optimization_results(study, window_idx)\n",
    "    # history_fig.write_html(f\"optimization_history_window_{window_idx}.html\")\n",
    "    # param_fig.write_html(f\"parameter_importance_window_{window_idx}.html\")\n",
    "    \n",
    "    # Save best parameters\n",
    "    best_params = study.best_params\n",
    "    \n",
    "    # Make predictions on test set\n",
    "    predictions = best_model.predict(test_df[feature_cols])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = evaluate_predictions(test_df['target'], predictions)\n",
    "    \n",
    "    # Store feature importance\n",
    "    global current_feature_importance\n",
    "    current_feature_importance = best_model.feature_importances_\n",
    "    \n",
    "    return metrics, predictions, best_params\n",
    "\n",
    "def rolling_forward_validation(df, feature_cols, train_days=5, test_days=1):\n",
    "    \"\"\"Perform rolling forward validation.\"\"\"\n",
    "    results = []\n",
    "    start_idx = 0\n",
    "    maxcnt = 0\n",
    "    while True or maxcnt < 5:\n",
    "        maxcnt += 1\n",
    "        # Get train/test split for current window\n",
    "        train_df, test_df, train_dates, test_dates = train_test_split_by_date(\n",
    "            df, train_days, test_days, start_idx\n",
    "        )\n",
    "        \n",
    "        if train_df is None:  # No more data\n",
    "            break\n",
    "            \n",
    "        # Train and evaluate\n",
    "        metrics, predictions, best_params = train_and_evaluate_window(\n",
    "            train_df, test_df, feature_cols, start_idx\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'train_start': min(train_dates),\n",
    "            'train_end': max(train_dates),\n",
    "            'test_start': min(test_dates),\n",
    "            'test_end': max(test_dates),\n",
    "            'metrics': metrics,\n",
    "            'predictions': predictions,\n",
    "            'best_params': best_params,\n",
    "            'actual_values': test_df['target'].values,\n",
    "            'dates': test_df['Open time'].values\n",
    "        })\n",
    "        \n",
    "        start_idx += test_days\n",
    "        \n",
    "    return results\n",
    "\n",
    "def calculate_aggregated_metrics(results):\n",
    "    \"\"\"Calculate aggregated metrics across all windows.\"\"\"\n",
    "    all_predictions = np.concatenate([r['predictions'] for r in results])\n",
    "    all_actuals = np.concatenate([r['actual_values'] for r in results])\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    overall_metrics = evaluate_predictions(all_actuals, all_predictions)\n",
    "    \n",
    "    # Calculate per-window metrics\n",
    "    window_metrics = pd.DataFrame([\n",
    "        {\n",
    "            'test_date': r['test_start'],\n",
    "            **r['metrics']\n",
    "        } for r in results\n",
    "    ])\n",
    "    \n",
    "    return {\n",
    "        'overall_metrics': overall_metrics,\n",
    "        'window_metrics': window_metrics\n",
    "    }\n",
    "\n",
    "def plot_metrics_over_time(window_metrics_df):\n",
    "    \"\"\"Plot performance metrics over time using Plotly.\"\"\"\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        subplot_titles=('Error Metrics Over Time', 'R² and Direction Accuracy Over Time'),\n",
    "        vertical_spacing=0.15\n",
    "    )\n",
    "\n",
    "    # Plot error metrics\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=window_metrics_df['test_date'], y=window_metrics_df['rmse'],\n",
    "                  name=\"RMSE\", mode='lines+markers'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=window_metrics_df['test_date'], y=window_metrics_df['mae'],\n",
    "                  name=\"MAE\", mode='lines+markers'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Plot R² and direction accuracy\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=window_metrics_df['test_date'], y=window_metrics_df['r2'],\n",
    "                  name=\"R²\", mode='lines+markers'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=window_metrics_df['test_date'], y=window_metrics_df['direction_accuracy'],\n",
    "                  name=\"Direction Accuracy\", mode='lines+markers'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        showlegend=True,\n",
    "        title_text=\"Model Performance Metrics Over Time\",\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"Test Date\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Test Date\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Error\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Score\", row=2, col=1)\n",
    "    \n",
    "    fig.write_html(\"metrics_over_time.html\")\n",
    "    return fig\n",
    "\n",
    "def plot_predictions_vs_actual(results):\n",
    "    \"\"\"Plot predictions vs actual values using Plotly.\"\"\"\n",
    "    all_dates = np.concatenate([r['dates'] for r in results])\n",
    "    all_predictions = np.concatenate([r['predictions'] for r in results])\n",
    "    all_actuals = np.concatenate([r['actual_values'] for r in results])\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=all_dates, y=all_actuals,\n",
    "                  name=\"Actual Returns\", mode='lines',\n",
    "                  line=dict(color='blue'))\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=all_dates, y=all_predictions,\n",
    "                  name=\"Predicted Returns\", mode='lines',\n",
    "                  line=dict(color='red'))\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Predicted vs Actual Returns Over Time',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Returns',\n",
    "        hovermode='x unified',\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig.write_html(\"predictions_vs_actual.html\")\n",
    "    return fig\n",
    "\n",
    "def plot_residuals(results):\n",
    "    \"\"\"Plot residuals analysis using Plotly.\"\"\"\n",
    "    all_predictions = np.concatenate([r['predictions'] for r in results])\n",
    "    all_actuals = np.concatenate([r['actual_values'] for r in results])\n",
    "    residuals = all_actuals - all_predictions\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=('Residuals Distribution', 'Residuals vs Predicted'),\n",
    "        specs=[[{\"type\": \"histogram\"}, {\"type\": \"scatter\"}]]\n",
    "    )\n",
    "    \n",
    "    # Residuals distribution\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=residuals, name=\"Residuals\",\n",
    "                    nbinsx=50),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Residuals vs Predicted\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=all_predictions, y=residuals,\n",
    "                  mode='markers', name=\"Residuals vs Predicted\",\n",
    "                  marker=dict(size=5, color='blue', opacity=0.5)),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=400,\n",
    "        title_text=\"Residuals Analysis\",\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.write_html(\"residuals_analysis.html\")\n",
    "    return fig\n",
    "\n",
    "def plot_feature_importance(feature_cols, importance_scores, top_n=10):\n",
    "    \"\"\"Plot feature importance scores using Plotly.\"\"\"\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': importance_scores\n",
    "    }).sort_values('importance', ascending=True)\n",
    "    \n",
    "    importance_df = importance_df.tail(top_n)\n",
    "    \n",
    "    fig = go.Figure(go.Bar(\n",
    "        x=importance_df['importance'],\n",
    "        y=importance_df['feature'],\n",
    "        orientation='h'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'Top {top_n} Most Important Features',\n",
    "        xaxis_title='Importance Score',\n",
    "        yaxis_title='Feature',\n",
    "        height=max(400, top_n * 25),\n",
    "        showlegend=False,\n",
    "        yaxis={'categoryorder': 'total ascending'}\n",
    "    )\n",
    "    \n",
    "    fig.write_html(\"feature_importance.html\")\n",
    "    return fig\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    df, feature_cols = load_and_preprocess_data('model_data.csv')\n",
    "    \n",
    "    # Perform rolling forward validation\n",
    "    results = rolling_forward_validation(df, feature_cols, train_days=5, test_days=1)\n",
    "    \n",
    "    # Calculate aggregated metrics\n",
    "    metrics = calculate_aggregated_metrics(results)\n",
    "    \n",
    "    # Print overall results\n",
    "    print(\"\\nOverall Metrics:\")\n",
    "    for metric, value in metrics['overall_metrics'].items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nPer-window Metrics Summary:\")\n",
    "    print(metrics['window_metrics'].describe())\n",
    "    \n",
    "    # Create and save visualizations\n",
    "    print(\"\\nGenerating and saving interactive plots...\")\n",
    "    \n",
    "    # 1. Metrics over time\n",
    "    metrics_plot = plot_metrics_over_time(metrics['window_metrics'])\n",
    "    \n",
    "    # 2. Predictions vs Actual\n",
    "    predictions_plot = plot_predictions_vs_actual(results)\n",
    "    \n",
    "    # 3. Residuals analysis\n",
    "    residuals_plot = plot_residuals(results)\n",
    "    \n",
    "    # 4. Feature importance\n",
    "    if 'current_feature_importance' in globals():\n",
    "        feature_imp_plot = plot_feature_importance(\n",
    "            feature_cols, \n",
    "            current_feature_importance\n",
    "        )\n",
    "    \n",
    "    print(\"Interactive HTML plots have been saved to the current directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Metrics:\n",
      "rmse: 0.2737\n",
      "mae: 0.2049\n",
      "r2: -0.5437\n",
      "mape: 11041884.0170\n",
      "direction_accuracy: 0.4804\n",
      "\n",
      "Per-window Metrics Summary:\n",
      "             rmse         mae          r2          mape  direction_accuracy\n",
      "count  216.000000  216.000000  216.000000  2.160000e+02          216.000000\n",
      "mean     0.249774    0.204853   -1.392180  1.104188e+07            0.480413\n",
      "std      0.112207    0.097230    3.114653  2.593207e+07            0.149168\n",
      "min      0.069436    0.056412  -35.917202  7.387412e+01            0.038462\n",
      "25%      0.169604    0.135464   -1.301220  1.607032e+02            0.384615\n",
      "50%      0.226256    0.183573   -0.602132  2.333839e+02            0.480769\n",
      "75%      0.319598    0.264382   -0.204980  2.384562e+06            0.576923\n",
      "max      0.734586    0.687925    0.551226  1.969526e+08            0.846154\n",
      "\n",
      "Generating and saving interactive plots...\n",
      "Interactive HTML plots have been saved to the current directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "pio.renderers.default = \"notebook\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"Load and preprocess the data.\"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert open_time to datetime\n",
    "    df['open_time'] = pd.to_datetime(df['open_time'])\n",
    "    \n",
    "    # Get feature columns (excluding open_time and target)\n",
    "    feature_cols = [col for col in df.columns if col.startswith('feature_')]\n",
    "    \n",
    "    return df, feature_cols\n",
    "\n",
    "def train_test_split_by_date(df, train_days, test_days, start_idx):\n",
    "    \"\"\"Split data into training and test sets based on dates.\"\"\"\n",
    "    all_dates = df['open_time'].dt.date.unique()\n",
    "    \n",
    "    if start_idx + train_days + test_days > len(all_dates):\n",
    "        return None, None, None, None\n",
    "    \n",
    "    train_dates = all_dates[start_idx:start_idx + train_days]\n",
    "    test_dates = all_dates[start_idx + train_days:start_idx + train_days + test_days]\n",
    "    \n",
    "    train_mask = df['open_time'].dt.date.isin(train_dates)\n",
    "    test_mask = df['open_time'].dt.date.isin(test_dates)\n",
    "    \n",
    "    return df[train_mask], df[test_mask], train_dates, test_dates\n",
    "\n",
    "def evaluate_predictions(y_true, y_pred):\n",
    "    \"\"\"Calculate regression metrics.\"\"\"\n",
    "    return {\n",
    "        'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'mae': mean_absolute_error(y_true, y_pred),\n",
    "        'r2': r2_score(y_true, y_pred),\n",
    "        'mape': np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100,  # Added small constant to avoid division by zero\n",
    "        'direction_accuracy': np.mean((y_true * y_pred) > 0)  # Direction prediction accuracy\n",
    "    }\n",
    "\n",
    "def create_model(trial):\n",
    "    \"\"\"Create a model with parameters suggested by Optuna.\"\"\"\n",
    "    return XGBRegressor(\n",
    "        n_estimators=trial.suggest_int('n_estimators', 50, 300),\n",
    "        max_depth=trial.suggest_int('max_depth', 2, 8),\n",
    "        learning_rate=trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
    "        min_child_weight=trial.suggest_float('min_child_weight', 1, 7),\n",
    "        subsample=trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        colsample_bytree=trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        gamma=trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "        reg_alpha=trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "        reg_lambda=trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "def optimize_parameters(train_df, val_df, feature_cols, n_trials=50):\n",
    "    \"\"\"Optimize hyperparameters using Optuna.\"\"\"\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df['target']\n",
    "    X_val = val_df[feature_cols]\n",
    "    y_val = val_df['target']\n",
    "    \n",
    "    def objective(trial):\n",
    "        model = create_model(trial)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            early_stopping_rounds=20,\n",
    "            verbose=False\n",
    "        )\n",
    "        preds = model.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "        return rmse\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "    \n",
    "    best_model = create_model(study.best_trial)\n",
    "    best_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    return best_model, study\n",
    "\n",
    "def plot_optimization_results(study, window_idx):\n",
    "    \"\"\"Plot optimization results using Plotly.\"\"\"\n",
    "    # Plot optimization history\n",
    "    history_fig = go.Figure()\n",
    "    history_data = {\n",
    "        'number': list(range(len(study.trials))),\n",
    "        'value': [t.value for t in study.trials]\n",
    "    }\n",
    "    \n",
    "    history_fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=history_data['number'],\n",
    "            y=history_data['value'],\n",
    "            mode='lines+markers',\n",
    "            name='RMSE'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    history_fig.update_layout(\n",
    "        title=f'Optimization History - Window {window_idx}',\n",
    "        xaxis_title='Trial',\n",
    "        yaxis_title='RMSE',\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    # Plot parameter importances\n",
    "    importance = optuna.importance.get_param_importances(study)\n",
    "    importance_df = pd.DataFrame({\n",
    "        'parameter': list(importance.keys()),\n",
    "        'importance': list(importance.values())\n",
    "    }).sort_values('importance', ascending=True)\n",
    "    \n",
    "    param_fig = go.Figure()\n",
    "    param_fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=importance_df['importance'],\n",
    "            y=importance_df['parameter'],\n",
    "            orientation='h'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    param_fig.update_layout(\n",
    "        title=f'Parameter Importances - Window {window_idx}',\n",
    "        xaxis_title='Importance',\n",
    "        yaxis_title='Parameter',\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    return history_fig, param_fig\n",
    "\n",
    "def train_and_evaluate_window(train_df, test_df, feature_cols, window_idx):\n",
    "    \"\"\"Train XGBoost and evaluate for a single window with hyperparameter optimization.\"\"\"\n",
    "    global current_feature_importance\n",
    "    \n",
    "    # Split train data into train and validation for optimization\n",
    "    train_dates = train_df['open_time'].dt.date.unique()\n",
    "    val_size = max(1, len(train_dates) // 5)  # 20% of training data for validation\n",
    "    \n",
    "    val_dates = train_dates[-val_size:]\n",
    "    train_dates = train_dates[:-val_size]\n",
    "    \n",
    "    train_mask = train_df['open_time'].dt.date.isin(train_dates)\n",
    "    val_mask = train_df['open_time'].dt.date.isin(val_dates)\n",
    "    \n",
    "    opt_train_df = train_df[train_mask]\n",
    "    val_df = train_df[val_mask]\n",
    "    \n",
    "    # Optimize parameters\n",
    "    best_model, study = optimize_parameters(opt_train_df, val_df, feature_cols)\n",
    "    \n",
    "    # Create optimization plots\n",
    "    history_fig, param_fig = plot_optimization_results(study, window_idx)\n",
    "    history_fig.write_html(f\"optimization_history_window_{window_idx}.html\")\n",
    "    param_fig.write_html(f\"parameter_importance_window_{window_idx}.html\")\n",
    "    \n",
    "    # Save best parameters\n",
    "    best_params = study.best_params\n",
    "    \n",
    "    # Make predictions on test set\n",
    "    predictions = best_model.predict(test_df[feature_cols])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = evaluate_predictions(test_df['target'], predictions)\n",
    "    \n",
    "    # Store feature importance\n",
    "    global current_feature_importance\n",
    "    current_feature_importance = best_model.feature_importances_\n",
    "    \n",
    "    return metrics, predictions, best_params\n",
    "\n",
    "def rolling_forward_validation(df, feature_cols, train_days=5, test_days=1):\n",
    "    \"\"\"Perform rolling forward validation.\"\"\"\n",
    "    results = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    while True:\n",
    "        # Get train/test split for current window\n",
    "        train_df, test_df, train_dates, test_dates = train_test_split_by_date(\n",
    "            df, train_days, test_days, start_idx\n",
    "        )\n",
    "        \n",
    "        if train_df is None:  # No more data\n",
    "            break\n",
    "            \n",
    "        # Train and evaluate\n",
    "        metrics, predictions, best_params = train_and_evaluate_window(\n",
    "            train_df, test_df, feature_cols, start_idx\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'train_start': min(train_dates),\n",
    "            'train_end': max(train_dates),\n",
    "            'test_start': min(test_dates),\n",
    "            'test_end': max(test_dates),\n",
    "            'metrics': metrics,\n",
    "            'predictions': predictions,\n",
    "            'best_params': best_params,\n",
    "            'actual_values': test_df['target'].values,\n",
    "            'dates': test_df['open_time'].values\n",
    "        })\n",
    "        \n",
    "        start_idx += test_days\n",
    "        \n",
    "    return results\n",
    "\n",
    "def calculate_aggregated_metrics(results):\n",
    "    \"\"\"Calculate aggregated metrics across all windows.\"\"\"\n",
    "    all_predictions = np.concatenate([r['predictions'] for r in results])\n",
    "    all_actuals = np.concatenate([r['actual_values'] for r in results])\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    overall_metrics = evaluate_predictions(all_actuals, all_predictions)\n",
    "    \n",
    "    # Calculate per-window metrics\n",
    "    window_metrics = pd.DataFrame([\n",
    "        {\n",
    "            'test_date': r['test_start'],\n",
    "            **r['metrics']\n",
    "        } for r in results\n",
    "    ])\n",
    "    \n",
    "    return {\n",
    "        'overall_metrics': overall_metrics,\n",
    "        'window_metrics': window_metrics\n",
    "    }\n",
    "\n",
    "def plot_metrics_over_time(window_metrics_df):\n",
    "    \"\"\"Plot performance metrics over time using Plotly.\"\"\"\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        subplot_titles=('Error Metrics Over Time', 'R² and Direction Accuracy Over Time'),\n",
    "        vertical_spacing=0.15\n",
    "    )\n",
    "\n",
    "    # Plot error metrics\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=window_metrics_df['test_date'], y=window_metrics_df['rmse'],\n",
    "                  name=\"RMSE\", mode='lines+markers'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=window_metrics_df['test_date'], y=window_metrics_df['mae'],\n",
    "                  name=\"MAE\", mode='lines+markers'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Plot R² and direction accuracy\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=window_metrics_df['test_date'], y=window_metrics_df['r2'],\n",
    "                  name=\"R²\", mode='lines+markers'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=window_metrics_df['test_date'], y=window_metrics_df['direction_accuracy'],\n",
    "                  name=\"Direction Accuracy\", mode='lines+markers'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        showlegend=True,\n",
    "        title_text=\"Model Performance Metrics Over Time\",\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"Test Date\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Test Date\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Error\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Score\", row=2, col=1)\n",
    "    \n",
    "    fig.write_html(\"metrics_over_time.html\")\n",
    "    return fig\n",
    "\n",
    "def plot_predictions_vs_actual(results):\n",
    "    \"\"\"Plot predictions vs actual values using Plotly.\"\"\"\n",
    "    all_dates = np.concatenate([r['dates'] for r in results])\n",
    "    all_predictions = np.concatenate([r['predictions'] for r in results])\n",
    "    all_actuals = np.concatenate([r['actual_values'] for r in results])\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=all_dates, y=all_actuals,\n",
    "                  name=\"Actual Returns\", mode='lines',\n",
    "                  line=dict(color='blue'))\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=all_dates, y=all_predictions,\n",
    "                  name=\"Predicted Returns\", mode='lines',\n",
    "                  line=dict(color='red'))\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Predicted vs Actual Returns Over Time',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Returns',\n",
    "        hovermode='x unified',\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig.write_html(\"predictions_vs_actual.html\")\n",
    "    return fig\n",
    "\n",
    "def plot_residuals(results):\n",
    "    \"\"\"Plot residuals analysis using Plotly.\"\"\"\n",
    "    all_predictions = np.concatenate([r['predictions'] for r in results])\n",
    "    all_actuals = np.concatenate([r['actual_values'] for r in results])\n",
    "    residuals = all_actuals - all_predictions\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=('Residuals Distribution', 'Residuals vs Predicted'),\n",
    "        specs=[[{\"type\": \"histogram\"}, {\"type\": \"scatter\"}]]\n",
    "    )\n",
    "    \n",
    "    # Residuals distribution\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=residuals, name=\"Residuals\",\n",
    "                    nbinsx=50),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Residuals vs Predicted\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=all_predictions, y=residuals,\n",
    "                  mode='markers', name=\"Residuals vs Predicted\",\n",
    "                  marker=dict(size=5, color='blue', opacity=0.5)),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=400,\n",
    "        title_text=\"Residuals Analysis\",\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.write_html(\"residuals_analysis.html\")\n",
    "    return fig\n",
    "\n",
    "def plot_feature_importance(feature_cols, importance_scores, top_n=10):\n",
    "    \"\"\"Plot feature importance scores using Plotly.\"\"\"\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': importance_scores\n",
    "    }).sort_values('importance', ascending=True)\n",
    "    \n",
    "    importance_df = importance_df.tail(top_n)\n",
    "    \n",
    "    fig = go.Figure(go.Bar(\n",
    "        x=importance_df['importance'],\n",
    "        y=importance_df['feature'],\n",
    "        orientation='h'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'Top {top_n} Most Important Features',\n",
    "        xaxis_title='Importance Score',\n",
    "        yaxis_title='Feature',\n",
    "        height=max(400, top_n * 25),\n",
    "        showlegend=False,\n",
    "        yaxis={'categoryorder': 'total ascending'}\n",
    "    )\n",
    "    \n",
    "    fig.write_html(\"feature_importance.html\")\n",
    "    return fig\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    df, feature_cols = load_and_preprocess_data('model_data.csv')\n",
    "    \n",
    "    # Perform rolling forward validation\n",
    "    results = rolling_forward_validation(df, feature_cols, train_days=5, test_days=1)\n",
    "    \n",
    "    # Calculate aggregated metrics\n",
    "    metrics = calculate_aggregated_metrics(results)\n",
    "    \n",
    "    # Print overall results\n",
    "    print(\"\\nOverall Metrics:\")\n",
    "    for metric, value in metrics['overall_metrics'].items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nPer-window Metrics Summary:\")\n",
    "    print(metrics['window_metrics'].describe())\n",
    "    \n",
    "    # Create and save visualizations\n",
    "    print(\"\\nGenerating and saving interactive plots...\")\n",
    "    \n",
    "    # 1. Metrics over time\n",
    "    metrics_plot = plot_metrics_over_time(metrics['window_metrics'])\n",
    "    \n",
    "    # 2. Predictions vs Actual\n",
    "    predictions_plot = plot_predictions_vs_actual(results)\n",
    "    \n",
    "    # 3. Residuals analysis\n",
    "    residuals_plot = plot_residuals(results)\n",
    "    \n",
    "    # 4. Feature importance\n",
    "    if 'current_feature_importance' in globals():\n",
    "        feature_imp_plot = plot_feature_importance(\n",
    "            feature_cols, \n",
    "            current_feature_importance\n",
    "        )\n",
    "    \n",
    "    # 5. Hyperparameter trends\n",
    "    hyperparameter_plot = plot_hyperparameter_trends(results)\n",
    "    \n",
    "    print(\"\\nHyperparameter Optimization Summary:\")\n",
    "    param_stats = pd.DataFrame([r['best_params'] for r in results]).describe()\n",
    "    print(param_stats)\n",
    "    \n",
    "    print(\"\\nInteractive HTML plots have been saved to the current directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
