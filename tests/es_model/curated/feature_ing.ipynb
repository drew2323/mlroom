{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to explore\n",
    "\n",
    "po konzultaci s jayjayem:\n",
    "\n",
    "15min has better correlations to daily returns\n",
    "15min created with tbbo data\n",
    "applyied rolling ma\n",
    "target is moving ma, which has a memory\n",
    "by retraining the model you are learning the ma and preserving the ma memory\n",
    "maybe use imbalance columns\n",
    "add as feature\n",
    "  * anchored daily vwap\n",
    "  * anchored daily ma (14)\n",
    "  * just ma as features\n",
    "  * forward looking MA as target (https://claude.ai/chat/69fbfe22-bc7f-4795-b964-15d10fbfca60)\n",
    "  * on MA as features/targets https://chatgpt.com/c/672d03bf-ded4-8013-8349-330786edc44f\n",
    "  * https://claude.ai/chat/69fbfe22-bc7f-4795-b964-15d10fbfca60 \n",
    "\n",
    "figure out how to turn to signals\n",
    "\n",
    "\n",
    "* enhance feature\n",
    "  * try add number of trades, inbalance columns into features\n",
    "  * try process use tbbo data for aggregation and use bbo data in ohlcv bars and build features upon it https://databento.com/docs/examples/basics-historical/midprice/example\n",
    "  * datetime features?\n",
    "* also keep idea of retreain the model frequently even intraday or find working interval (30+1)\n",
    "\n",
    "This creates features and tests the model and output features to csv. Upon this csv prediction potential can be checked with other notebook and also cgb reg a class run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class LibraryTradingModel:\n",
    "    def __init__(self, train_days=10, test_days=1, forward_bars=5, n_classes=5):\n",
    "        \"\"\"\n",
    "        Initialize the trading model.\n",
    "        \n",
    "        Parameters:\n",
    "        train_days (int): Number of days for training window\n",
    "        test_days (int): Number of days for testing window\n",
    "        forward_bars (int): Number of bars to look ahead for returns\n",
    "        n_classes (int): Number of classes to create (using percentiles)\n",
    "        \"\"\"\n",
    "        self.train_days = train_days\n",
    "        self.test_days = test_days\n",
    "        self.forward_bars = forward_bars\n",
    "        self.n_classes = n_classes\n",
    "        self.scaler = StandardScaler()\n",
    "        self.class_boundaries = None\n",
    "    \n",
    "    def get_date_windows(self, data):\n",
    "        \"\"\"\n",
    "        Calculate date windows for training and testing.\n",
    "        Handles any frequency of data by using datetime index.\n",
    "        \n",
    "        Returns:\n",
    "        List of tuples: (train_start, train_end, test_start, test_end)\n",
    "        \"\"\"\n",
    "        # Convert days to timedelta\n",
    "        train_delta = timedelta(days=self.train_days)\n",
    "        test_delta = timedelta(days=self.test_days)\n",
    "        \n",
    "        start_date = data.index[0]\n",
    "        end_date = data.index[-1]\n",
    "        \n",
    "        windows = []\n",
    "        current_start = start_date\n",
    "        \n",
    "        while True:\n",
    "            train_end = current_start + train_delta\n",
    "            test_start = train_end\n",
    "            test_end = test_start + test_delta\n",
    "            \n",
    "            # Break if we've reached the end of data\n",
    "            if test_end > end_date:\n",
    "                break\n",
    "                \n",
    "            windows.append((current_start, train_end, test_start, test_end))\n",
    "            current_start = current_start + timedelta(days=self.test_days)  # Move forward by test period\n",
    "            \n",
    "        return windows\n",
    "    \n",
    "    def prepare_features(self, df):\n",
    "        \"\"\"\n",
    "        Create features from the price data.\n",
    "        Added log returns to handle small price changes better.\n",
    "        \"\"\"\n",
    "        features = df.copy()\n",
    "        \n",
    "        # Log returns for better handling of small price changes\n",
    "        features['log_return'] = np.log(features['close'] / features['close'].shift(1))\n",
    "        \n",
    "        # Regular returns\n",
    "        features['returns_1'] = features['close'].pct_change()\n",
    "        features['returns_5'] = features['close'].pct_change(5)\n",
    "        features['returns_20'] = features['close'].pct_change(20)\n",
    "        \n",
    "        # Log-based features\n",
    "        features['log_return_5'] = features['log_return'].rolling(5).sum()\n",
    "        features['log_return_20'] = features['log_return'].rolling(20).sum()\n",
    "        \n",
    "        # Moving averages\n",
    "        features['sma_5'] = features['close'].rolling(5).mean()\n",
    "        features['sma_20'] = features['close'].rolling(20).mean()\n",
    "        \n",
    "        # Volatility (using log returns for better normality)\n",
    "        features['volatility_5'] = features['log_return'].rolling(5).std()\n",
    "        features['volatility_20'] = features['log_return'].rolling(20).std()\n",
    "        \n",
    "        # Price relative to moving averages\n",
    "        features['price_rel_sma5'] = features['close'] / features['sma_5']\n",
    "        features['price_rel_sma20'] = features['close'] / features['sma_20']\n",
    "        \n",
    "        # Drop NaN values\n",
    "        features = features.dropna()\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def create_target(self, df, train_data=None):\n",
    "        \"\"\"\n",
    "        Create target variable based on forward returns.\n",
    "        Enhanced to handle zero returns and ensure consistent class numbers.\n",
    "        \"\"\"\n",
    "        # Calculate forward returns\n",
    "        forward_returns = df['close'].shift(-self.forward_bars) / df['close'] - 1\n",
    "        \n",
    "        if train_data is None:\n",
    "            # This is training data - calculate class boundaries\n",
    "            returns_data = forward_returns.dropna()\n",
    "            \n",
    "            # Calculate initial percentile values\n",
    "            percentiles = np.linspace(0, 100, self.n_classes + 1)\n",
    "            initial_boundaries = np.percentile(returns_data, percentiles)\n",
    "            \n",
    "            # Handle zero returns specially\n",
    "            zero_return_idx = np.where(initial_boundaries == 0)[0]\n",
    "            if len(zero_return_idx) > 1:\n",
    "                # If multiple zeros, distribute them evenly\n",
    "                zero_start = zero_return_idx[0]\n",
    "                zero_end = zero_return_idx[-1]\n",
    "                \n",
    "                # Create small spreads around zero\n",
    "                epsilon = 1e-10\n",
    "                spreads = np.linspace(-epsilon, epsilon, zero_end - zero_start + 1)\n",
    "                initial_boundaries[zero_start:zero_end + 1] = spreads\n",
    "            \n",
    "            # Ensure uniqueness and proper spacing\n",
    "            unique_boundaries = []\n",
    "            epsilon = 1e-10\n",
    "            \n",
    "            for i, boundary in enumerate(initial_boundaries):\n",
    "                if i == 0:  # First boundary\n",
    "                    unique_boundaries.append(boundary - epsilon)\n",
    "                elif i == len(initial_boundaries) - 1:  # Last boundary\n",
    "                    unique_boundaries.append(boundary + epsilon)\n",
    "                else:  # Middle boundaries\n",
    "                    # Ensure spacing from previous boundary\n",
    "                    prev_boundary = unique_boundaries[-1]\n",
    "                    if abs(boundary - prev_boundary) < epsilon:\n",
    "                        boundary = prev_boundary + epsilon\n",
    "                    unique_boundaries.append(boundary)\n",
    "            \n",
    "            self.class_boundaries = np.array(unique_boundaries)\n",
    "            \n",
    "            # Print distribution info for debugging\n",
    "            print(\"\\nClass Boundaries Distribution:\")\n",
    "            class_counts = []\n",
    "            for i in range(len(self.class_boundaries)-1):\n",
    "                count = ((returns_data >= self.class_boundaries[i]) & \n",
    "                        (returns_data < self.class_boundaries[i+1])).sum()\n",
    "                pct = count / len(returns_data) * 100\n",
    "                class_counts.append(count)\n",
    "                print(f\"Class {i}: {self.class_boundaries[i]:.8f} to {self.class_boundaries[i+1]:.8f} \"\n",
    "                    f\"(count: {count}, {pct:.2f}%)\")\n",
    "            \n",
    "            # Warning if any class is empty\n",
    "            min_class_count = min(class_counts)\n",
    "            if min_class_count == 0:\n",
    "                print(\"\\nWARNING: Some classes have zero samples!\")\n",
    "                \n",
    "        # Ensure we have class boundaries\n",
    "        if self.class_boundaries is None:\n",
    "            raise ValueError(\"Class boundaries not set. Must process training data first.\")\n",
    "        \n",
    "        # Classify returns into classes\n",
    "        try:\n",
    "            classes = pd.cut(forward_returns, \n",
    "                            bins=self.class_boundaries, \n",
    "                            labels=range(self.n_classes),\n",
    "                            include_lowest=True)\n",
    "            \n",
    "            # Verify we have all classes represented\n",
    "            unique_classes = classes.dropna().unique()\n",
    "            if len(unique_classes) < self.n_classes:\n",
    "                print(f\"\\nWARNING: Only {len(unique_classes)} classes present in data\")\n",
    "                print(\"Classes found:\", sorted(unique_classes))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in classification: {str(e)}\")\n",
    "            print(\"Forward returns stats:\")\n",
    "            print(forward_returns.describe())\n",
    "            print(\"\\nClass boundaries:\")\n",
    "            print(self.class_boundaries)\n",
    "            raise\n",
    "        \n",
    "        return classes\n",
    "\n",
    "    \n",
    "    def prepare_data(self, df, train_data=None):\n",
    "        \"\"\"\n",
    "        Prepare features and target for modeling.\n",
    "        \"\"\"\n",
    "        # Prepare features\n",
    "        features_df = self.prepare_features(df)\n",
    "        \n",
    "        # Create target\n",
    "        target = self.create_target(features_df, train_data if train_data is not None else None)\n",
    "        \n",
    "        # Select feature columns\n",
    "        feature_cols = ['log_return', 'returns_1', 'returns_5', 'returns_20',\n",
    "                    'log_return_5', 'log_return_20',\n",
    "                    'volatility_5', 'volatility_20',\n",
    "                    'price_rel_sma5', 'price_rel_sma20']\n",
    "        \n",
    "        X = features_df[feature_cols]\n",
    "        y = target\n",
    "        \n",
    "        # Scale features\n",
    "        if train_data is None:\n",
    "            X_scaled = self.scaler.fit_transform(X)\n",
    "        else:\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        # Convert to DataFrame to keep column names\n",
    "        X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "        \n",
    "        return X_scaled, y\n",
    "    \n",
    "    def display_confusion_matrix(self, y_true, y_pred, title):\n",
    "        \"\"\"\n",
    "        Display confusion matrix with detailed information.\n",
    "        \"\"\"\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "        \n",
    "        labels = [[f'{int(cm[i, j])}\\n{cm_percent[i, j]:.1f}%' \n",
    "                  if cm[i, j] > 0 else ''\n",
    "                  for j in range(len(cm))]\n",
    "                 for i in range(len(cm))]\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=labels, fmt='', cmap='Blues')\n",
    "        plt.title(f'Confusion Matrix: {title}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.show()\n",
    "        \n",
    "        # Print additional metrics\n",
    "        print(\"\\nPer-class metrics:\")\n",
    "        precision = np.diag(cm) / np.sum(cm, axis=0)\n",
    "        recall = np.diag(cm) / np.sum(cm, axis=1)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        metrics_df = pd.DataFrame({\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1\n",
    "        }, index=[f'Class {i}' for i in range(self.n_classes)])\n",
    "        \n",
    "        print(metrics_df.round(3))\n",
    "        print(f\"\\nOverall Accuracy: {np.sum(np.diag(cm)) / np.sum(cm):.3f}\")\n",
    "\n",
    "    def run_iteration(self, train_data, test_data, iteration_num):\n",
    "        \"\"\"\n",
    "        Run a single iteration of training and testing.\n",
    "        Enhanced error handling and class validation.\n",
    "        \"\"\"\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        \n",
    "        try:\n",
    "            # Reset class boundaries for each iteration\n",
    "            self.class_boundaries = None\n",
    "            \n",
    "            print(f\"\\nProcessing iteration {iteration_num}\")\n",
    "            print(f\"Training data: {train_data.index[0]} to {train_data.index[-1]}\")\n",
    "            print(f\"Test data: {test_data.index[0]} to {test_data.index[-1]}\")\n",
    "            \n",
    "            # Prepare training data\n",
    "            X_train, y_train = self.prepare_data(train_data)\n",
    "            mask_train = ~y_train.isna()\n",
    "            X_train = X_train[mask_train]\n",
    "            y_train = y_train[mask_train]\n",
    "            \n",
    "            if len(X_train) < self.forward_bars + 1:\n",
    "                print(f\"Warning: Iteration {iteration_num} - Insufficient training data\")\n",
    "                return None, None\n",
    "                \n",
    "            print(f\"Training samples after preparation: {len(X_train)}\")\n",
    "            \n",
    "            # Validate class distribution\n",
    "            class_dist = y_train.value_counts(normalize=True).sort_index()\n",
    "            print(f\"Class distribution in training:\")\n",
    "            print(class_dist)\n",
    "            \n",
    "            # Check if we have all classes\n",
    "            missing_classes = set(range(self.n_classes)) - set(class_dist.index)\n",
    "            if missing_classes:\n",
    "                print(f\"\\nWARNING: Missing classes in training data: {missing_classes}\")\n",
    "                print(\"Adjusting model to handle missing classes...\")\n",
    "                # Add one sample for each missing class to ensure model can handle all classes\n",
    "                for missing_class in missing_classes:\n",
    "                    X_train = pd.concat([X_train, X_train.iloc[[0]]])\n",
    "                    y_train = pd.concat([y_train, pd.Series([missing_class], index=[X_train.index[-1]])])\n",
    "            \n",
    "            # Train the model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Prepare test data\n",
    "            X_test, y_test = self.prepare_data(test_data, train_data)\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions = model.predict(X_test)\n",
    "            probabilities = model.predict_proba(X_test)\n",
    "            \n",
    "            # Validate probabilities shape\n",
    "            if probabilities.shape[1] != self.n_classes:\n",
    "                raise ValueError(f\"Probability matrix has wrong shape: {probabilities.shape}\")\n",
    "            \n",
    "            # Create results DataFrame\n",
    "            results = pd.DataFrame({\n",
    "                'predicted_class': predictions,\n",
    "                'actual_class': y_test\n",
    "            }, index=X_test.index)\n",
    "            \n",
    "            # Add probability for each class\n",
    "            for i in range(self.n_classes):\n",
    "                results[f'prob_class_{i}'] = probabilities[:, i]\n",
    "            \n",
    "            # Display iteration results\n",
    "            mask = ~results['actual_class'].isna()\n",
    "            if mask.any():\n",
    "                print(f\"\\nValid test samples: {mask.sum()}\")\n",
    "                self.display_confusion_matrix(\n",
    "                    results.loc[mask, 'actual_class'],\n",
    "                    results.loc[mask, 'predicted_class'],\n",
    "                    f'Iteration {iteration_num}'\n",
    "                )\n",
    "            \n",
    "            return results, model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in iteration {iteration_num}: {str(e)}\")\n",
    "            print(f\"Training data shape: {train_data.shape}\")\n",
    "            print(f\"Test data shape: {test_data.shape}\")\n",
    "            return None, None\n",
    "\n",
    "    def run_rolling_window(self, data, num_iterations=None):\n",
    "        \"\"\"\n",
    "        Run the model using a rolling window approach.\n",
    "        Handles any frequency of data using datetime index.\n",
    "        \"\"\"\n",
    "        # Get all possible windows\n",
    "        windows = self.get_date_windows(data)\n",
    "        \n",
    "        if num_iterations is not None:\n",
    "            windows = windows[:num_iterations]\n",
    "        \n",
    "        all_results = {}\n",
    "        all_predictions = []\n",
    "        all_actuals = []\n",
    "        \n",
    "        for i, (train_start, train_end, test_start, test_end) in enumerate(windows):\n",
    "            # Get data for this iteration using datetime indexing\n",
    "            train_mask = (data.index >= train_start) & (data.index < train_end)\n",
    "            test_mask = (data.index >= test_start) & (data.index < test_end)\n",
    "            \n",
    "            train_data = data[train_mask]\n",
    "            test_data = data[test_mask]\n",
    "            \n",
    "            # Skip if not enough data\n",
    "            min_required_bars = max(20, self.forward_bars + 1)  # minimum bars needed for features\n",
    "            if len(train_data) < min_required_bars or len(test_data) < 1:\n",
    "                print(f\"Skipping iteration {i}: Insufficient data (train: {len(train_data)}, test: {len(test_data)} bars)\")\n",
    "                continue\n",
    "            \n",
    "            results, model = self.run_iteration(train_data, test_data, i)\n",
    "            \n",
    "            if results is not None:\n",
    "                all_results[i] = {\n",
    "                    'train_period': (train_start, train_end),\n",
    "                    'test_period': (test_start, test_end),\n",
    "                    'results': results,\n",
    "                    'model': model\n",
    "                }\n",
    "                \n",
    "                mask = ~results['actual_class'].isna()\n",
    "                if mask.any():\n",
    "                    all_predictions.extend(results.loc[mask, 'predicted_class'])\n",
    "                    all_actuals.extend(results.loc[mask, 'actual_class'])\n",
    "        \n",
    "        # Display overall results\n",
    "        if all_predictions:\n",
    "            print(\"\\n=== Overall Results Across All Iterations ===\")\n",
    "            self.display_confusion_matrix(\n",
    "                all_actuals,\n",
    "                all_predictions,\n",
    "                'All Iterations Combined'\n",
    "            )\n",
    "        elif not all_results:\n",
    "            print(\"No valid results generated across all iterations.\")\n",
    "\n",
    "        return all_results\n",
    "\n",
    "    def generate_features_csv(self, data, output_filename, use_raw_target=True):\n",
    "        \"\"\"\n",
    "        Generate raw (unscaled) features and target variables for the entire dataset and save to CSV.\n",
    "        \n",
    "        Parameters:\n",
    "        data (pd.DataFrame): Input data with required columns (close, etc.)\n",
    "        output_filename (str): Name of the output CSV file\n",
    "        use_raw_target (bool): If True, use raw forward returns as target instead of classes\n",
    "        \n",
    "        Returns:\n",
    "        pd.DataFrame: DataFrame containing all features and target variables\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Preparing features for entire dataset...\")\n",
    "            \n",
    "            # Prepare features\n",
    "            features_df = self.prepare_features(data)\n",
    "            \n",
    "            # Calculate raw forward returns\n",
    "            forward_returns = features_df['close'].shift(-self.forward_bars) / features_df['close'] - 1\n",
    "            \n",
    "            # If using classified target, create classes\n",
    "            if not use_raw_target:\n",
    "                self.class_boundaries = None  # Reset class boundaries\n",
    "                target = self.create_target(features_df)\n",
    "            else:\n",
    "                target = forward_returns\n",
    "            \n",
    "            # Select feature columns (same as in prepare_data method)\n",
    "            feature_cols = [\n",
    "                'log_return', 'returns_1', 'returns_5', 'returns_20',\n",
    "                'log_return_5', 'log_return_20',\n",
    "                'volatility_5', 'volatility_20',\n",
    "                'price_rel_sma5', 'price_rel_sma20'\n",
    "            ]\n",
    "            \n",
    "            # Get raw features (no scaling)\n",
    "            X = features_df[feature_cols]\n",
    "            \n",
    "            # Rename columns to match desired output format\n",
    "            output_df = pd.DataFrame(X.values, columns=[f'feature_{i}' for i in range(X.shape[1])], \n",
    "                                index=X.index)\n",
    "            \n",
    "            # Add the target variable\n",
    "            output_df['target'] = target\n",
    "            \n",
    "            # Add the timestamp as a column instead of index\n",
    "            output_df.reset_index(inplace=True)\n",
    "            output_df.rename(columns={'index': 'Open time'}, inplace=True)\n",
    "            \n",
    "            # Remove rows with NaN values (these would be from the initial rolling windows \n",
    "            # and forward-looking target calculation)\n",
    "            output_df = output_df.dropna()\n",
    "            \n",
    "            # Save to CSV\n",
    "            output_df.to_csv(output_filename, index=False)\n",
    "            \n",
    "            print(f\"\\nFeatures and target generated successfully!\")\n",
    "            print(f\"Output saved to: {output_filename}\")\n",
    "            print(f\"Total rows: {len(output_df)}\")\n",
    "            print(f\"Features shape: {X.shape}\")\n",
    "            print(f\"Target type: {'Raw forward returns' if use_raw_target else 'Classified'}\")\n",
    "            print(\"\\nFeature columns:\")\n",
    "            for i, col in enumerate(feature_cols):\n",
    "                print(f\"feature_{i}: {col}\")\n",
    "            \n",
    "            # Print some basic statistics about the features\n",
    "            print(\"\\nFeature statistics:\")\n",
    "            stats_df = X.describe()\n",
    "            print(stats_df)\n",
    "                \n",
    "            return output_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating features: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTOOLS: Loaded env variables from file /Users/davidbrazda/Documents/Development/python/.env\n"
     ]
    }
   ],
   "source": [
    "import vectorbtpro as vbt\n",
    "import ttools as tts\n",
    "#from lightweight_charts import chart, Panel, PlotDFAccessor, PlotSRAccessor\n",
    "#import talib\n",
    "import ttools as tts\n",
    "from ttools.config import DATA_DIR\n",
    "from ttools.utils import zoneNY, AggType\n",
    "from ttools.loaders import load_data\n",
    "from numba import jit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "vbt.settings.plotting.auto_rangebreaks = True\n",
    "vbt.settings.set_theme(\"dark\")\n",
    "vbt.settings.plotting[\"use_resampler\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched agg files 1\n",
      "\n",
      "File: BAC-AggType.OHLCV-1-2024-01-15T09-30-00-2024-10-20T16-00-00-4679BCFMOPUVWZ-100-False.parquet\n",
      "Coverage: 2024-01-15 09:30:00 to 2024-10-20 16:00:00\n",
      "Symbol: BAC\n",
      "Agg Type: AggType.OHLCV\n",
      "Resolution: 1\n",
      "Excludes: 4679BCFMOPUVWZ\n",
      "Minsize: 100\n",
      "Main Session Only: False\n",
      "--------------------------------------------------------------------------------\n",
      "Loaded from agg_cache /Users/davidbrazda/Library/Application Support/v2realbot/aggcache/BAC-AggType.OHLCV-1-2024-01-15T09-30-00-2024-10-20T16-00-00-4679BCFMOPUVWZ-100-False.parquet\n"
     ]
    }
   ],
   "source": [
    "#This is how to call LOAD function\n",
    "symbol = [\"BAC\"]\n",
    "#datetime in zoneNY \n",
    "day_start = datetime(2024, 10, 1, 9, 30, 0)\n",
    "day_stop = datetime(2024, 10, 20, 16, 0, 0)\n",
    "day_start = zoneNY.localize(day_start)\n",
    "day_stop = zoneNY.localize(day_stop)\n",
    "\n",
    "#requested AGG\n",
    "resolution = 1 #12s bars\n",
    "agg_type = AggType.OHLCV #other types AggType.OHLCV_VOL, AggType.OHLCV_DOL, AggType.OHLCV_RENKO\n",
    "exclude_conditions = ['C','O','4','B','7','V','P','W','U','Z','F','9','M','6'] #None to defaults\n",
    "minsize = 100 #min trade size to include\n",
    "main_session_only = False\n",
    "force_remote = False\n",
    "\n",
    "bac_data = load_data(symbol = symbol,\n",
    "                     agg_type = agg_type,\n",
    "                     resolution = resolution,\n",
    "                     start_date = day_start,\n",
    "                     end_date = day_stop,\n",
    "                     #exclude_conditions = None,\n",
    "                     minsize = minsize,\n",
    "                     main_session_only = main_session_only,\n",
    "                     force_remote = force_remote,\n",
    "                     return_vbt = False, #returns vbt object\n",
    "                     verbose = True\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = bac_data[\"BAC\"].between_time(\"9:30\", \"16:00\").copy()\n",
    "\n",
    "model = LibraryTradingModel(train_days=5, test_days=1, forward_bars=3, n_classes=3)\n",
    "\n",
    "features_df = model.generate_features_csv(df, \"features_output.csv\", use_raw_target=True) #raw or classed\n",
    "\n",
    "#results = model.run_rolling_window(df, num_iterations=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-10-01 09:30:22-04:00</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>-0.001265</td>\n",
       "      <td>2.533891e-04</td>\n",
       "      <td>-1.265983e-03</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>1.000406</td>\n",
       "      <td>1.000479</td>\n",
       "      <td>0.000507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-01 09:30:24-04:00</td>\n",
       "      <td>-0.000253</td>\n",
       "      <td>-0.000253</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>-0.000760</td>\n",
       "      <td>3.802040e-04</td>\n",
       "      <td>-7.599747e-04</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>1.000076</td>\n",
       "      <td>1.000263</td>\n",
       "      <td>0.000760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-10-01 09:30:25-04:00</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>7.855958e-04</td>\n",
       "      <td>4.053917e-04</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>1.000324</td>\n",
       "      <td>1.000649</td>\n",
       "      <td>0.000228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-10-01 09:30:28-04:00</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>1.013428e-03</td>\n",
       "      <td>8.866933e-04</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>1.000476</td>\n",
       "      <td>1.000959</td>\n",
       "      <td>-0.000253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-10-01 09:30:29-04:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>7.599747e-04</td>\n",
       "      <td>7.599747e-04</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>1.000324</td>\n",
       "      <td>1.000921</td>\n",
       "      <td>-0.000410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155705</th>\n",
       "      <td>2024-10-18 15:59:52-04:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000236</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-2.362949e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000006</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155706</th>\n",
       "      <td>2024-10-18 15:59:53-04:00</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>-0.000354</td>\n",
       "      <td>-1.181684e-04</td>\n",
       "      <td>-3.544633e-04</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.999905</td>\n",
       "      <td>0.999906</td>\n",
       "      <td>0.000236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155707</th>\n",
       "      <td>2024-10-18 15:59:54-04:00</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>4.323256e-17</td>\n",
       "      <td>1.181684e-04</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>1.000024</td>\n",
       "      <td>1.000018</td>\n",
       "      <td>0.000354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155708</th>\n",
       "      <td>2024-10-18 15:59:55-04:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.323256e-17</td>\n",
       "      <td>1.260385e-16</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>1.000024</td>\n",
       "      <td>1.000018</td>\n",
       "      <td>0.000168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155709</th>\n",
       "      <td>2024-10-18 15:59:56-04:00</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>1.181544e-04</td>\n",
       "      <td>1.181544e-04</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>1.000118</td>\n",
       "      <td>1.000130</td>\n",
       "      <td>0.000236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155710 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            time  feature_0  feature_1  feature_2  feature_3  \\\n",
       "0      2024-10-01 09:30:22-04:00   0.000253   0.000253   0.000253  -0.001265   \n",
       "1      2024-10-01 09:30:24-04:00  -0.000253  -0.000253   0.000380  -0.000760   \n",
       "2      2024-10-01 09:30:25-04:00   0.000405   0.000405   0.000786   0.000405   \n",
       "3      2024-10-01 09:30:28-04:00   0.000355   0.000355   0.001014   0.000887   \n",
       "4      2024-10-01 09:30:29-04:00   0.000000   0.000000   0.000760   0.000760   \n",
       "...                          ...        ...        ...        ...        ...   \n",
       "155705 2024-10-18 15:59:52-04:00   0.000000   0.000000   0.000000  -0.000236   \n",
       "155706 2024-10-18 15:59:53-04:00  -0.000118  -0.000118  -0.000118  -0.000354   \n",
       "155707 2024-10-18 15:59:54-04:00   0.000118   0.000118   0.000000   0.000118   \n",
       "155708 2024-10-18 15:59:55-04:00   0.000000   0.000000   0.000000   0.000000   \n",
       "155709 2024-10-18 15:59:56-04:00   0.000118   0.000118   0.000118   0.000118   \n",
       "\n",
       "           feature_4     feature_5  feature_6  feature_7  feature_8  \\\n",
       "0       2.533891e-04 -1.265983e-03   0.000263   0.000392   1.000406   \n",
       "1       3.802040e-04 -7.599747e-04   0.000212   0.000359   1.000076   \n",
       "2       7.855958e-04  4.053917e-04   0.000250   0.000329   1.000324   \n",
       "3       1.013428e-03  8.866933e-04   0.000263   0.000336   1.000476   \n",
       "4       7.599747e-04  7.599747e-04   0.000275   0.000335   1.000324   \n",
       "...              ...           ...        ...        ...        ...   \n",
       "155705  0.000000e+00 -2.362949e-04   0.000000   0.000132   1.000000   \n",
       "155706 -1.181684e-04 -3.544633e-04   0.000053   0.000134   0.999905   \n",
       "155707  4.323256e-17  1.181684e-04   0.000084   0.000111   1.000024   \n",
       "155708  4.323256e-17  1.260385e-16   0.000084   0.000108   1.000024   \n",
       "155709  1.181544e-04  1.181544e-04   0.000099   0.000111   1.000118   \n",
       "\n",
       "        feature_9    target  \n",
       "0        1.000479  0.000507  \n",
       "1        1.000263  0.000760  \n",
       "2        1.000649  0.000228  \n",
       "3        1.000959 -0.000253  \n",
       "4        1.000921 -0.000410  \n",
       "...           ...       ...  \n",
       "155705   1.000006  0.000000  \n",
       "155706   0.999906  0.000236  \n",
       "155707   1.000018  0.000354  \n",
       "155708   1.000018  0.000168  \n",
       "155709   1.000130  0.000236  \n",
       "\n",
       "[155710 rows x 12 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
