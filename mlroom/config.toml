    #download_server = "142.132.188.109"
    #upload_server = "142.132.188.109"
    #download_server = "0.0.0.0"
    #upload_server = "0.0.0.0"
    upload_server = "5.161.179.223" #where to upload model
    download_server = "5.161.179.223" #to download batches/runners
    upload = true #nyni se uploaduje po kazdem batchi
    save_best = false #stores/uploads best but not last, not the final model but checkpointed (model with best val_loss according to checkpoint)

[train]
    #runners = ["fd29c97a-a141-4917-88de-078b045758c8"]
    batch = "536c5570"
    remove_cross_sequences = true #nyni nefunguje, je true
    runners_per_batch = 999 #how many runners per batch

[validation] #toto je externi(nasledna) validace (SCALAR(rt test) a VECTOR)
    #runners = ["fd29c97a-a141-4917-88de-078b045758c8"] #runners or batch as sources
    batch = "53e04a2a" #nad konkretnim batchem
    validate_during_fit = true #posila externi validacni data (runnery/batche) do fit jako validaiton_data pro zobrazeni loss, zatim neimplementovano, pokud neni tak se bere z fit.validation_split
    #test_size = 0 #DECOMM - zda se maji splitnout X data pro externi validaci (pozor bere data na trenink)
    #send_val_data_to_fit

[fit] #params that go to Model.fit
    batch_size = 32
    epochs = 4
    validation_split = 0 #validace po kazde z epoch (ubira z testovacich dat
    verbose = 'auto' # 0 = silent, 1 = progress bar, 2 = one line per epoch

    #these params are used only for validation_data
    #validation_steps = 
    #validation_batch_size =
    #validation_freq =

# ModelCheckpoint to save the model with the lowest validation loss
# [fit.callbacks.ModelCheckpoint] ## Load the best_model = load_model('best_model.h5')
#     filepath = 'checkpoint.model.keras' #"{epoch:02d}-{val_loss:.2f}.keras"
#     monitor='val_loss'
#     mode='min' #max
#     save_best_only=true #save best or save each epoch
#     verbose=1

# [fit.callbacks.EarlyStopping]
#     monitor='val_loss' #'mean_squared_error' #val_loss
#     patience=20
#     min_delta=0.001
#     mode='min'
# [fit.callbacks.ReduceLROnPlateau] #reduces lr
#     #monitor='mean_squared_error'
#     monitor='val_loss'
#     factor=0.1 #snizi learning rate o 10% kdyz se po 5 epoch nesnizi val_loss
#     patience=5
#     min_lr=0.00001
#     verbose=1

#TODO dopracovat moznost vlozit externi validacni data do trainu
#TODO vyzkouset Conv1D2Inputs_ - ma tam i 0.2737 val_mean, vyzkouset na batche (val_mean_squared_error: 0.2593)
#TODO vyzkouset samostatny runner to fit validate_to

[model]
     #vsechny ceny logreturn test
    name = "Trans2D" #name that would be used to predict
    version = 0.1
    note = "Test transformer s categorical entropy, 0,1,2 labeling and log_returns"

#this is the target that will be used for label
#we support target resampling, if used on other than highest resolution
[model.target] #target used for labeling, we support target resampling, if used on other than highest resolution
    indicators = "targetema"
    target_reference = "tick_price"
    #scaler = "MinMaxScaler" #pokud zakomentovano, nescalujeme
[model.input.highres]
    cbar_indicators = ["tick_volume","tick_trades", "daytimetick", "tick_price_log_return", "tick_price"]
    sequence_length = 100
    scaler = "StandardScaler"
[model.input.lowres]
    bars = ["trades","vwap","close","open","high","low"]
    indicators = ["daytimebar", "divema", "slope5", "voldiv", "rsi14MA", "emaSlow","vwap_log_return","close_log_return","open_log_return","high_log_return","low_log_return"]
    sequence_length = 100
    scaler = "StandardScaler"
# [model.input.daily]
#     dailyBars = ["rsi", "volume_sma_divergence", "open", "close"]
#     sequence_length = 25
#     scaler = "StandardScaler"
[model.architecture]
    name = "Transformer2Inputs_" #"Conv1D2Inputs_" #"Conv1DLSTM2InputsAtt_"  #Conv1D3Inputs_ LSTM3Inputs_ #Conv1D_ "TCN_" "Conv1DcomplexAtt_"  name of file/function with architecture
[model.architecture.params] #params that are passed into model function (needed to compile)
    learning_rate = 0.0001 #0.00005
    trans_layers = [2,2] #number of transformer layers for each input
#    l2_reg = 0.001 # Added L2 regularization parameter



#TODO batches bude treba predelat
#zejmena promyslet checkpointovani v ramci batchu