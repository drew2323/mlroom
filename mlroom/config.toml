    #download_server = "142.132.188.109"
    #upload_server = "142.132.188.109"
    #download_server = "0.0.0.0"
    #upload_server = "0.0.0.0"

    #test 5.161.179.223
    upload_server = "5.161.179.223" #where to upload model
    download_server = "5.161.179.223" #to download batches/runners
    upload = false
[validation]
    test_size = 0.05 #validujeme na casti testovacich dat
    runners = ["fde1d7ac-d0e5-4473-b366-657807b7690c"] #nad seznamem runner
    batch = "" #nad konkretnim batchem
[model]
    name = "LSTM"
    version = 0.23
    note = "LSTM no att, just 3 inputs, added l2 regularization"
    #train_runner_ids = ["4a4757b2-1cb5-41a1-a950-190aa6a0a196"]
    train_batch_id = "1ca02a05"
    train_epochs = 60
    train_remove_cross_sequences = true #nyni nefunguje, je true
    train_batch_size = 64
    #sem pridat batch_id a take earlystoppage params

#state.bars, state.indicators, state.cbar_idnicators, state.dailyBars, state.static_inds
#each input groups the same granularity
#first is always highest with target, now - in the future can be in the lowest and is distributed to high-res

#TODO pozor na poradi v predictu musi odpovidat

#this is the target that will be used for label
#we support target resampling, if used on other than highest resolution
[model.target] 
    cbar_indicators = "target3"
    target_reference = "tick_price"
    scaler = "StandardScaler"

[model.input.highres]
    cbar_indicators = ["tick_price", "tick_volume","tick_trades"] #"market_time"
    sequence_length = 75
    scaler = "StandardScaler"
[model.input.lowres]
    bars = ["volume", "close", "open", "high", "low","vwap","trades"]
    indicators = ["atr10"]
    sequence_length = 200
    scaler = "StandardScaler"
[model.input.daily]
    dailyBars = ["close", "vwap", "open", "volume", "trades"]
    sequence_length = 25
    scaler = "StandardScaler"

[model.architecture]
    name = "LSTM3Inputs_"  #Conv1D3Inputs_ LSTM3Inputs_ #Conv1D_ "TCN_" "Conv1DcomplexAtt_"  name of file/function with architecture
[model.architecture.params]
    learning_rate = 0.001
    l2_reg = 0.001 # Added L2 regularization parameter
[model.architecture.params.callbacks.EarlyStopping]
    #monitor='mean_squared_error'
    monitor='loss'
    patience=20
    min_delta=0.0001
    mode='min'
[model.architecture.params.callbacks.ReduceLROnPlateau] #reduces lr
    #monitor='mean_squared_error'
    monitor='val_loss'
    factor=0.1 #snizi learning rate o 10% kdyz se po 5 epoch nesnizi val_loss
    patience=5
    min_lr=0.00001
    verbose=1

#novy format?
# [train.sources]
#     #train_runner_ids = ["96fad44c-7a7a-4833-8915-6cc5180593a0"]
#     train_batch_id = "b3346be9"
# [train.sequencing]
#     train_remove_cross_sequences = true
# [train.params]
#     epochs = 1
#     batch_size = 32
