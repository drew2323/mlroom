    #download_server = "142.132.188.109"
    #upload_server = "142.132.188.109"
    #download_server = "0.0.0.0"
    #upload_server = "0.0.0.0"
    upload_server = "5.161.179.223" #where to upload model
    download_server = "5.161.179.223" #to download batches/runners
    upload = true
    save_best = false #stores/uploads best but not last, not the final model but checkpointed (model with best val_loss according to checkpoint)

#tato sekce bude pridana
# [train]
#     #train_runner_ids = ["7cf6fe32-7003-4179-8149-9236a906c733"]
#     train_batch_id = "b110ccd6"
#     train_remove_cross_sequences = true #nyni nefunguje, je true
#     save_checkpointed = true

[validation] #toto je externi(nasledna) validace (SCALAR(rt test) a VECTOR)
    test_size = 0 #zda se maji splitnout X data pro externi validaci (pozor bere data na trenink)
    runners = ["fd29c97a-a141-4917-88de-078b045758c8"] #runners or batch as sources
    batch = "" #nad konkretnim batchem
    validate_during_fit = true #posila externi validacni data (runnery/batche) do fit jako validaiton_data pro zobrazeni loss, zatim neimplementovano, pokud neni tak se bere z fit.validation_split
    #send_val_data_to_fit

[fit] #params that go to Model.fit
    batch_size = 32
    epochs = 5
    validation_split = 0.2 #validace po kazde z epoch (ubira z testovacich dat
    verbose = 'auto' # 0 = silent, 1 = progress bar, 2 = one line per epoch
#TODO eval val_loss mechanism

# ModelCheckpoint to save the model with the lowest validation loss
# [fit.callbacks.ModelCheckpoint] ## Load the best_model = load_model('best_model.h5')
#     filepath = 'checkpoint.model.keras' #"{epoch:02d}-{val_loss:.2f}.keras"
#     monitor='val_loss'
#     mode='min' #max
#     save_best_only=true #save best or save each epoch
#     verbose=1

# [fit.callbacks.EarlyStopping]
#     monitor='val_loss' #'mean_squared_error' #val_loss
#     patience=20
#     min_delta=0.001
#     mode='min'
# [fit.callbacks.ReduceLROnPlateau] #reduces lr
#     #monitor='mean_squared_error'
#     monitor='val_loss'
#     factor=0.1 #snizi learning rate o 10% kdyz se po 5 epoch nesnizi val_loss
#     patience=5
#     min_lr=0.00001
#     verbose=1

#TODO dopracovat moznost vlozit externi validacni data do trainu
#TODO vyzkouset Conv1D2Inputs_ - ma tam i 0.2737 val_mean, vyzkouset na batche (val_mean_squared_error: 0.2593)
#TODO vyzkouset samostatny runner to fit validate_to

[model]
     #vsechny ceny logreturn test
    name = "TransD2inputs" #name that would be used to predict
    version = 0.1
    note = "Test transformer s categorical entropy, 0,1,2 labeling and log_returns"
    #train_runner_ids = ["fd29c97a-a141-4917-88de-078b045758c8"]
    train_batch_id = "536c5570"
    train_remove_cross_sequences = true #nyni nefunguje, je true

#this is the target that will be used for label
#we support target resampling, if used on other than highest resolution
[model.target] #target used for labeling, we support target resampling, if used on other than highest resolution
    indicators = "targetema"
    target_reference = "tick_price"
    #scaler = "MinMaxScaler" #pokud zakomentovano, nescalujeme
[model.input.highres]
    cbar_indicators = ["tick_volume","tick_trades", "daytimetick", "tick_price_log_return", "tick_price"]
    sequence_length = 100
    scaler = "StandardScaler"
[model.input.lowres]
    bars = ["trades","vwap","close","open","high","low"]
    indicators = ["daytimebar", "divema", "slope5", "voldiv", "rsi14MA", "emaSlow","vwap_log_return","close_log_return","open_log_return","high_log_return","low_log_return"]
    sequence_length = 300
    scaler = "StandardScaler"
# [model.input.daily]
#     dailyBars = ["rsi", "volume_sma_divergence", "open", "close"]
#     sequence_length = 25
#     scaler = "StandardScaler"
[model.architecture]
    name = "Transformer2Inputs_" #"Conv1D2Inputs_" #"Conv1DLSTM2InputsAtt_"  #Conv1D3Inputs_ LSTM3Inputs_ #Conv1D_ "TCN_" "Conv1DcomplexAtt_"  name of file/function with architecture
[model.architecture.params] #params that are passed into model function (needed to compile)
    learning_rate = 0.0001 #0.00005
    trans_layers = [1,1] #number of transformer layers for each input
#    l2_reg = 0.001 # Added L2 regularization parameter

#novy format?
# [train.sources]
#     #train_runner_ids = ["96fad44c-7a7a-4833-8915-6cc5180593a0"]
#     train_batch_id = "b3346be9"
# [train.sequencing]
#     train_remove_cross_sequences = true
# [train.params]
#     epochs = 1
#     batch_size = 32
