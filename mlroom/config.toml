    #download_server = "142.132.188.109"
    #upload_server = "142.132.188.109"
    #download_server = "0.0.0.0"
    #upload_server = "0.0.0.0"
    upload_server = "5.161.179.223" #where to upload model
    download_server = "5.161.179.223" #to download batches/runners
    upload = true #nyni se uploaduje po kazdem batchi
    save_best = true #stores/uploads best but not last, not the final model but checkpointed (model with best val_loss according to checkpoint)


# [cache] #for remote SCP of prepared data (save/load)
#     remote_host = '5.161.179.223'
#     username = 'david'
#     private_key_path = '~/.ssh/id_ed25519'
#     remote_path = '/home/david/'  # Remote path where the file will be located

[train]
    runners = ["878af6c2-f19c-45cf-a220-a241e9cce04e"]
    #batch = "c1cf7114"
    remove_cross_sequences = true #nyni nefunguje, je true
    runners_per_batch = 9999 #how many runners per batch

[validation] #toto je externi(nasledna) validace (SCALAR(rt test) a VECTOR)
    runners = ["878af6c2-f19c-45cf-a220-a241e9cce04e","d9a5f888-014b-40ce-a806-22c8b2603757","ab0f089c-6512-4c9e-8263-a98c7a216956"] #runners or batch as sources
    #batch = "53e04a2a" #nad konkretnim batchem
    validate_during_fit = true #posila externi validacni data (runnery/batche) do fit jako validaiton_data pro zobrazeni loss, zatim neimplementovano, pokud neni tak se bere z fit.validation_split
    #test_size = 0 #DECOMM - zda se maji splitnout X data pro externi validaci (pozor bere data na trenink)
    #send_val_data_to_fit

[fit] #params that go to Model.fit
    batch_size = 128
    epochs = 1
    validation_split = 0 #validace po kazde z epoch (ubira z testovacich dat
    verbose = 'auto' # 0 = silent, 1 = progress bar, 2 = one line per epoch
    
    #class_weight = "balanced" #can be dict(goes directly to fit) or string - then it calculates balanced weights
    #{0: 2.2578978447003246, 1: 0.434418314019541, 2: 3.918780425313861} pripadne 0 a 2 prohozena
    class_weight.0 = 5.5
    class_weight.1 = 0.1
    class_weight.2 = 5.5
    #these params are used only for validation_data
    #validation_steps = 
    #validation_batch_size =
    #validation_freq =

# ModelCheckpoint to save the model with the lowest validation loss
# [fit.callbacks.ModelCheckpoint] ## Load the best_model = load_model('best_model.h5')
#     filepath = 'checkpoint.model.keras' #"{epoch:02d}-{val_loss:.2f}.keras"
#     monitor='val_loss'
#     mode='min' #max
#     save_best_only=true #save best or save each epoch
#     verbose=1

# [fit.callbacks.EarlyStopping]
#     monitor='val_loss' #'mean_squared_error' #val_loss
#     patience=20
#     min_delta=0.001
#     mode='min'
# [fit.callbacks.ReduceLROnPlateau] #reduces lr
#     #monitor='mean_squared_error'
#     monitor='val_loss'
#     factor=0.1 #snizi learning rate o 10% kdyz se po 5 epoch nesnizi val_loss
#     patience=5
#     min_lr=0.00001
#     verbose=1

#TODO dopracovat moznost vlozit externi validacni data do trainu
#TODO vyzkouset Conv1D2Inputs_ - ma tam i 0.2737 val_mean, vyzkouset na batche (val_mean_squared_error: 0.2593)
#TODO vyzkouset samostatny runner to fit validate_to

[model]
     #vsechny ceny logreturn test
    name = "TransTune" #name that would be used to predict
    version = 0.3
    note = "Classweights testes"

#this is the target that will be used for label
#we support target resampling, if used on other than highest resolution
[model.target] #target used for labeling, we support target resampling, if used on other than highest resolution
    indicators = "targetema"
    target_reference = "tick_price"
    #scaler = "MinMaxScaler" #pokud zakomentovano, nescalujeme
[model.input.highres]
    cbar_indicators = ["tick_volume","tick_trades", "daytimetick", "tick_price_log_return", "tick_price"]
    sequence_length = 100
    scaler = "StandardScaler"
[model.input.lowres]
    bars = ["trades","vwap","close","open","high","low"]
    indicators = ["daytimebar", "divema", "slope5", "voldiv", "rsi14MA", "emaSlow","vwap_log_return","close_log_return","open_log_return","high_log_return","low_log_return"]
    sequence_length = 200
    scaler = "StandardScaler"
# [model.input.daily]
#     dailyBars = ["rsi", "volume_sma_divergence", "open", "close"]
#     sequence_length = 25
#     scaler = "StandardScaler"
[model.architecture]
    name = "Transformer2Inputs_" #"Conv1D2Inputs_" #"Conv1DLSTM2InputsAtt_"  #Conv1D3Inputs_ LSTM3Inputs_ #Conv1D_ "TCN_" "Conv1DcomplexAtt_"  name of file/function with architecture
[model.architecture.params] #params that are passed into model function (needed to compile)
    learning_rate = 0.001 #0.00005
    trans_layers = [1,1] #number of transformer layers for each input
    l2_reg = 0.001 # Added L2 regularization parameter



#TODO batches bude treba predelat
#zejmena promyslet checkpointovani v ramci batchu